{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n",
    "\n",
    "# Load accuracy metric for model evaluation\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def train_model_albert(model, tokenizer, tokenized_train, tokenized_valid, out_dir, lr, batch_size, epochs):\n",
    "    \"\"\"\n",
    "    Trains the ALBERT model on the specified training and validation datasets.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The ALBERT model to train.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for ALBERT.\n",
    "        tokenized_train (Dataset): Tokenized training dataset.\n",
    "        tokenized_valid (Dataset): Tokenized validation dataset.\n",
    "        out_dir (str): Directory to save the trained model.\n",
    "        lr (float): Learning rate for training.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The trained model's Trainer instance for further use.\n",
    "    \"\"\"\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "    # Define Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics_albert\n",
    "    )\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def apply_pruning_albert(model, amount, steps):\n",
    "    \"\"\"\n",
    "    Applies iterative pruning on the model's linear layers.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The ALBERT model to prune.\n",
    "        amount (float): Fraction of weights to prune in each step.\n",
    "        steps (int): Number of pruning iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for step in range(steps):\n",
    "        print(f\"Pruning step {step + 1}/{steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        print(f\"Pruning step {step + 1} completed.\")\n",
    "\n",
    "def fine_tune_pruned_model_albert(model, tokenizer, tokenized_train, tokenized_valid, out_dir, lr, batch_size, epochs):\n",
    "    \"\"\"\n",
    "    Fine-tunes a pruned ALBERT model on the training and validation datasets.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The pruned ALBERT model to fine-tune.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for ALBERT.\n",
    "        tokenized_train (Dataset): Tokenized training dataset.\n",
    "        tokenized_valid (Dataset): Tokenized validation dataset.\n",
    "        out_dir (str): Directory to save the fine-tuned model.\n",
    "        lr (float): Learning rate for fine-tuning.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        epochs (int): Number of epochs to fine-tune the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The fine-tuned model's Trainer instance for further use.\n",
    "    \"\"\"\n",
    "    # Fine-tuning training arguments\n",
    "    fine_tuning_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=fine_tuning_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics_albert\n",
    "    )\n",
    "\n",
    "    print(\"Starting fine-tuning on pruned model...\")\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def compute_metrics_albert(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy for model evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing model predictions and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with computed accuracy score.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def evaluate_and_infer_albert(trainer, model, tokenizer, tokenized_test):\n",
    "    \"\"\"\n",
    "    Evaluates the pruned ALBERT model on the test set and performs inference.\n",
    "\n",
    "    Args:\n",
    "        trainer (Trainer): Trainer object for the pruned ALBERT model.\n",
    "        model (transformers.PreTrainedModel): The pruned and fine-tuned ALBERT model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for ALBERT.\n",
    "        tokenized_test (Dataset): Tokenized test dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference pipeline\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    \n",
    "    for file_name in all_files:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                content = file.read()\n",
    "                result = classify(content)\n",
    "                print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "def run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=50,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline for training, pruning, fine-tuning, and evaluating the ALBERT model.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the Hugging Face dataset to use.\n",
    "        model_name (str): Pre-trained ALBERT model name.\n",
    "        out_dir (str): Output directory for saving model checkpoints.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for parallel tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for both training and fine-tuning.\n",
    "        pruning_amount (float): Fraction of weights to prune in each pruning step.\n",
    "        pruning_steps (int): Number of pruning steps to apply.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load dataset splits\n",
    "    train_dataset = load_dataset(dataset_name, split='train[:95%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split='validation[:100%]')\n",
    "    test_dataset = load_dataset(dataset_name, split='test[:100%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=11, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Tokenize datasets\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    # Phase 1: Train the model\n",
    "    trainer = train_model_albert(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenized_train=tokenized_train,\n",
    "        tokenized_valid=tokenized_valid,\n",
    "        out_dir=out_dir,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Phase 2: Apply pruning\n",
    "    apply_pruning_albert(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Phase 3: Fine-tune the pruned model\n",
    "    trainer = fine_tune_pruned_model_albert(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenized_train=tokenized_train,\n",
    "        tokenized_valid=tokenized_valid,\n",
    "        out_dir=out_dir,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Save the pruned and fine-tuned model\n",
    "    model.save_pretrained('pruned_albert_finetuned')\n",
    "    tokenizer.save_pretrained('pruned_albert_finetuned')\n",
    "\n",
    "    # Load pruned model for evaluation\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_albert_finetuned')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_albert_finetuned')\n",
    "\n",
    "    # Final Evaluation and Inference\n",
    "    evaluate_and_infer_albert(trainer, model, tokenizer, tokenized_test)\n",
    "\n",
    "# ALBERT Call the function with custom arguments\n",
    "run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=50,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n",
    "\n",
    "def dynamic_prune_model_distilbert(model, trainer, prune_amount=0.1, prune_steps=3):\n",
    "    \"\"\"\n",
    "    Applies iterative dynamic pruning on the model's linear layers during training.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The DistilBERT model to prune.\n",
    "        trainer (Trainer): The Trainer instance for managing the training process.\n",
    "        prune_amount (float): Fraction of weights to prune in each step.\n",
    "        prune_steps (int): Number of pruning iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None: The model is pruned in place and the pruning is applied dynamically.\n",
    "    \"\"\"\n",
    "    for step in range(prune_steps):\n",
    "        print(f\"Pruning step {step + 1}/{prune_steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "        # Fine-tune the model after pruning\n",
    "        trainer.train()\n",
    "        # Remove pruning and make it permanent\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "def train_distilbert_model(dataset_name, train_split, valid_split, model_name, \n",
    "                           batch_size, num_procs, lr, epochs, out_dir):\n",
    "    \"\"\"\n",
    "    Trains the DistilBERT model on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        model_name (str): Name of the pretrained DistilBERT model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the trained model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    valid_dataset = load_dataset(dataset_name, split=valid_split)\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenizes the input text examples.\"\"\"\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"\n",
    "        Computes accuracy for model evaluation.\n",
    "\n",
    "        Args:\n",
    "            eval_pred (tuple): Tuple containing model predictions and true labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with computed accuracy score.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "def fine_tune_pruned_model_DistilBERT(dataset_name, train_split, valid_split, model, tokenizer, \n",
    "                           batch_size, num_procs, lr, epochs, out_dir):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pruned DistilBERT model.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        model (transformers.PreTrainedModel): The pruned DistilBERT model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for fine-tuning.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    valid_dataset = load_dataset(dataset_name, split=valid_split)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenizes the input text examples.\"\"\"\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"\n",
    "        Computes accuracy for model evaluation.\n",
    "\n",
    "        Args:\n",
    "            eval_pred (tuple): Tuple containing model predictions and true labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with computed accuracy score.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def run_distilbert_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    train_split='train[:95%]',\n",
    "    valid_split='validation[:100%]',\n",
    "    test_split='test[:100%]',\n",
    "    model_name='distilbert-base-uncased',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=1,\n",
    "    prune_amount=0.1,\n",
    "    prune_steps=3,\n",
    "    out_dir='arxiv_distilbert'\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes the DistilBERT model pipeline including dataset loading, tokenization,\n",
    "    model training, dynamic pruning, evaluation, and inference.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        test_split (str): Slice of the test data.\n",
    "        model_name (str): Name of the pretrained DistilBERT model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        prune_amount (float): Fraction of weights to prune.\n",
    "        prune_steps (int): Number of pruning steps.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Train the DistilBERT model\n",
    "    trainer, model, tokenizer = train_distilbert_model(\n",
    "        dataset_name, train_split, valid_split, model_name, \n",
    "        batch_size, num_procs, lr, epochs, out_dir\n",
    "    )\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model_distilbert(model, trainer, prune_amount, prune_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_distilbert')\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    fine_tune_pruned_model_DistilBERT(\n",
    "        dataset_name, train_split, valid_split, model, tokenizer,\n",
    "        batch_size, num_procs, lr, epochs, 'fine_tuned_pruned_model'\n",
    "    )\n",
    "\n",
    "    # Load the fine-tuned pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('fine_tuned_pruned_model')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('fine_tuned_pruned_model')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the fine-tuned model on the test dataset\n",
    "    test_dataset = load_dataset(dataset_name, split=test_split)\n",
    "    tokenized_test = test_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True), batched=True, num_proc=num_procs)\n",
    "    \n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after fine-tuning: {eval_result}\")\n",
    "\n",
    "    # Inference with the fine-tuned pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                content = file.read()\n",
    "                result = classify(content)\n",
    "                print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Call the function with specified arguments\n",
    "run_distilbert_pipeline(\n",
    "  dataset_name=\"ccdv/arxiv-classification\", \n",
    "  train_split='train[:95%]', \n",
    "  valid_split='validation[:85%]', \n",
    "  test_split='test[:85%]', \n",
    "  model_name='distilbert-base-uncased', \n",
    "  batch_size=32, \n",
    "  num_procs=4, \n",
    "  lr=0.00005, \n",
    "  epochs=50, \n",
    "  prune_amount=0.1, \n",
    "  prune_steps=3, \n",
    "  out_dir='arxiv_distilbert'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n",
    "\n",
    "def train_roberta_model(model_name: str, \n",
    "                        dataset_name: str, \n",
    "                        sample_fraction: float, \n",
    "                        out_dir: str, \n",
    "                        batch_size: int = 16, \n",
    "                        num_epochs: int = 5, \n",
    "                        learning_rate: float = 5e-5):\n",
    "    \"\"\"\n",
    "    Trains the RoBERTa model on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained RoBERTa model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the trained model.\n",
    "        AutoModelForSequenceClassification: The trained model.\n",
    "        AutoTokenizer: The tokenizer used for the model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "def dynamic_prune_roberta_model(model, trainer, amount=0.1, steps=3):\n",
    "    \"\"\"\n",
    "    Applies iterative dynamic pruning on the model's linear layers during training.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The RoBERTa model to prune.\n",
    "        trainer (Trainer): The Trainer instance for managing the training process.\n",
    "        amount (float): Fraction of weights to prune in each step.\n",
    "        steps (int): Number of pruning iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None: The model is pruned in place and the pruning is applied dynamically.\n",
    "    \"\"\"\n",
    "    for step in range(steps):\n",
    "        print(f\"Pruning step {step + 1}/{steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        # Fine-tune the model after pruning\n",
    "        trainer.train()\n",
    "        # Remove pruning and make it permanent\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "def fine_tune_pruned_roberta_model(model, tokenizer, dataset_name, sample_fraction, out_dir, \n",
    "                                    batch_size=16, num_epochs=5, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pruned RoBERTa model.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The pruned RoBERTa model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for fine-tuning.\n",
    "        out_dir (str): Directory to save the fine-tuned model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Tokenization\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def roberta_pipeline(\n",
    "    model_name: str, \n",
    "    dataset_name: str, \n",
    "    sample_fraction: float, \n",
    "    out_dir: str, \n",
    "    batch_size: int = 16, \n",
    "    num_epochs: int = 5, \n",
    "    learning_rate: float = 5e-5, \n",
    "    pruning_amount: float = 0.1, \n",
    "    pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Executes the RoBERTa model pipeline including dataset loading, training,\n",
    "    dynamic pruning, and fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained RoBERTa model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Train the RoBERTa model\n",
    "    trainer, model, tokenizer = train_roberta_model(model_name, dataset_name, sample_fraction, out_dir, batch_size, num_epochs, learning_rate)\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_roberta_model(model, trainer, pruning_amount, pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for fine-tuning\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    fine_tune_trainer = fine_tune_pruned_roberta_model(model, tokenizer, dataset_name, sample_fraction, f'fine_tuned_{out_dir}', batch_size, num_epochs, learning_rate)\n",
    "\n",
    "    # Evaluate the pruned and fine-tuned model on the test dataset\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "    eval_result = fine_tune_trainer.evaluate(test_dataset)\n",
    "    print(f\"Evaluation results after pruning and fine-tuning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned and fine-tuned model\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "\n",
    "# Main execution for RoBERTa\n",
    "roberta_pipeline(\n",
    "    model_name='roberta-base',\n",
    "    dataset_name='ccdv/arxiv-classification',\n",
    "    sample_fraction=0.95,  # Use 95% of training data\n",
    "    out_dir='arxiv_roberta',\n",
    "    batch_size=16,\n",
    "    num_epochs=50,\n",
    "    learning_rate=5e-5,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n",
    "\n",
    "def train_bert_model(model_name: str, \n",
    "                     dataset_name: str, \n",
    "                     sample_fraction: float, \n",
    "                     out_dir: str, \n",
    "                     batch_size: int = 32, \n",
    "                     num_epochs: int = 3, \n",
    "                     learning_rate: float = 5e-5):\n",
    "    \"\"\"\n",
    "    Trains the BERT model on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained BERT model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the trained model.\n",
    "        AutoModelForSequenceClassification: The trained model.\n",
    "        AutoTokenizer: The tokenizer used for the model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    return trainer, model, tokenizer\n",
    "\n",
    "def dynamic_prune_bert_model(model, trainer, amount=0.1, steps=3):\n",
    "    \"\"\"\n",
    "    Applies iterative dynamic pruning on the model's linear layers during training.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The BERT model to prune.\n",
    "        trainer (Trainer): The Trainer instance for managing the training process.\n",
    "        amount (float): Fraction of weights to prune in each step. Default is 0.1.\n",
    "        steps (int): Number of pruning iterations to perform. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        None: The model is pruned in place and the pruning is applied dynamically.\n",
    "    \"\"\"\n",
    "    for step in range(steps):\n",
    "        print(f\"Pruning step {step + 1}/{steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        # Fine-tune the model after pruning\n",
    "        trainer.train()\n",
    "        # Remove pruning and make it permanent\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "def fine_tune_pruned_bert_model(model, tokenizer, dataset_name, sample_fraction, out_dir, \n",
    "                                 batch_size=32, num_epochs=3, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pruned BERT model.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The pruned BERT model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for the model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for fine-tuning.\n",
    "        out_dir (str): Directory to save the fine-tuned model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Tokenization\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def bert_pipeline(\n",
    "    model_name: str, \n",
    "    dataset_name: str, \n",
    "    sample_fraction: float, \n",
    "    out_dir: str, \n",
    "    batch_size: int = 32, \n",
    "    num_epochs: int = 3, \n",
    "    learning_rate: float = 5e-5, \n",
    "    pruning_amount: float = 0.1, \n",
    "    pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Executes the BERT model pipeline including dataset loading, training,\n",
    "    dynamic pruning, and fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained BERT model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Train the BERT model\n",
    "    trainer, model, tokenizer = train_bert_model(model_name, dataset_name, sample_fraction, out_dir, batch_size, num_epochs, learning_rate)\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_bert_model(model, trainer, pruning_amount, pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    fine_tune_trainer = fine_tune_pruned_bert_model(model, tokenizer, dataset_name, sample_fraction, f'fine_tuned_{out_dir}', batch_size, num_epochs, learning_rate)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "    eval_result = fine_tune_trainer.evaluate(test_dataset)\n",
    "    print(f\"Evaluation results after pruning and fine-tuning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "\n",
    "# Main execution for BERT\n",
    "bert_pipeline(\n",
    "    model_name='bert-base-uncased',\n",
    "    dataset_name='ccdv/arxiv-classification',\n",
    "    sample_fraction=0.8,\n",
    "    out_dir='arxiv_bert',\n",
    "    batch_size=32,\n",
    "    num_epochs=50,\n",
    "    learning_rate=5e-5,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "def load_and_preprocess_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version (str): Version of the dataset.\n",
    "        sample_train_size (int): Size of the training sample.\n",
    "        sample_val_size (int): Size of the validation sample.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (train_dataset, validation_dataset)\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    \n",
    "    return train_data, validation_data\n",
    "\n",
    "def tokenize_data(train_data, validation_data):\n",
    "    \"\"\"\n",
    "    Tokenizes the training and validation datasets.\n",
    "\n",
    "    Args:\n",
    "        train_data: The training dataset.\n",
    "        validation_data: The validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (train_dataset, validation_dataset)\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True).set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True).set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "def train_gpt2_model(train_dataset, validation_dataset, output_dir: str):\n",
    "    \"\"\"\n",
    "    Trains the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: The tokenized training dataset.\n",
    "        validation_dataset: The tokenized validation dataset.\n",
    "        output_dir (str): Directory to save the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the trained model.\n",
    "        GPT2LMHeadModel: The trained GPT-2 model.\n",
    "    \"\"\"\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    return trainer, model\n",
    "\n",
    "def compute_head_importance(model, validation_dataset):\n",
    "    \"\"\"\n",
    "    Computes the importance of attention heads.\n",
    "\n",
    "    Args:\n",
    "        model (GPT2LMHeadModel): The GPT-2 model.\n",
    "        validation_dataset: The validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Head importance scores for each layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    head_importance = {layer: [0.0] * model.config.n_head for layer in range(model.config.n_layer)}\n",
    "    dataloader = validation_dataset\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "def prune_gpt2_heads(model, head_importance):\n",
    "    \"\"\"\n",
    "    Prunes heads based on importance scores.\n",
    "\n",
    "    Args:\n",
    "        model (GPT2LMHeadModel): The GPT-2 model.\n",
    "        head_importance (Dict): Head importance scores for each layer.\n",
    "\n",
    "    Returns:\n",
    "        None: The model is pruned in place.\n",
    "    \"\"\"\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < 0.2]\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - 1:\n",
    "            heads_to_prune[layer] = []  # Skip pruning if too many heads would be removed\n",
    "\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "def fine_tune_pruned_gpt2_model(model, train_dataset, validation_dataset, output_dir: str):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pruned GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        model (GPT2LMHeadModel): The pruned GPT-2 model.\n",
    "        train_dataset: The tokenized training dataset.\n",
    "        validation_dataset: The tokenized validation dataset.\n",
    "        output_dir (str): Directory to save the fine-tuned model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: The Trainer instance for the fine-tuned model.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def evaluate_gpt2_model(trainer, validation_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        trainer (Trainer): The Trainer instance for the model.\n",
    "        validation_dataset: The validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Perplexity of the model on the validation dataset.\n",
    "    \"\"\"\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "def generate_text(model, tokenizer, input_text: str):\n",
    "    \"\"\"\n",
    "    Generates text using the trained GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        model (GPT2LMHeadModel): The GPT-2 model.\n",
    "        tokenizer (GPT2Tokenizer): The tokenizer used for the model.\n",
    "        input_text (str): Input text for generation.\n",
    "\n",
    "    Returns:\n",
    "        List: Generated texts.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "def gpt2_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str, input_text: str):\n",
    "    \"\"\"\n",
    "    Complete GPT-2 pipeline including loading data, training, pruning, evaluation, and text generation.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version (str): Version of the dataset.\n",
    "        sample_train_size (int): Size of the training sample.\n",
    "        sample_val_size (int): Size of the validation sample.\n",
    "        output_dir (str): Directory to save the model.\n",
    "        input_text (str): Input text for generation.\n",
    "\n",
    "    Returns:\n",
    "        List: Generated texts.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    train_data, validation_data = load_and_preprocess_data(dataset_name, version, sample_train_size, sample_val_size)\n",
    "\n",
    "    # Tokenize data\n",
    "    train_dataset, validation_dataset = tokenize_data(train_data, validation_data)\n",
    "\n",
    "    # Train the GPT-2 model\n",
    "    trainer, model = train_gpt2_model(train_dataset, validation_dataset, output_dir)\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance(model, validation_dataset)\n",
    "\n",
    "    # Prune heads based on importance\n",
    "    prune_gpt2_heads(model, head_importance)\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    fine_tune_trainer = fine_tune_pruned_gpt2_model(model, train_dataset, validation_dataset, output_dir)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_gpt2_model(fine_tune_trainer, validation_dataset)\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Text generation\n",
    "    generated_texts = generate_text(model, GPT2Tokenizer.from_pretrained('gpt2'), input_text)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "generated = gpt2_pipeline(\n",
    "        dataset_name=\"cnn_dailymail\",\n",
    "        version=\"3.0.0\",\n",
    "        sample_train_size=27000,\n",
    "        sample_val_size=10000,\n",
    "        output_dir=\"./results-gpt2-automated-pruning\",\n",
    "        input_text=\"The future of AI in healthcare is\"\n",
    "    )\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "def load_and_preprocess_t5_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the T5 dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., \"cnn_dailymail\").\n",
    "        version (str): Version of the dataset to load.\n",
    "        sample_train_size (int): Number of training samples to use.\n",
    "        sample_val_size (int): Number of validation samples to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Training and validation datasets.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"input_text\": x['article'], \"target_text\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    \n",
    "    print(f\"T5 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    \n",
    "    return train_data, validation_data\n",
    "\n",
    "def tokenize_t5_data(train_data, validation_data):\n",
    "    \"\"\"\n",
    "    Tokenizes the T5 data.\n",
    "\n",
    "    Args:\n",
    "        train_data: The training dataset.\n",
    "        validation_data: The validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tokenized training and validation datasets.\n",
    "    \"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(examples['input_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        labels = tokenizer(examples['target_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "        return inputs\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "def train_t5_model(train_dataset, validation_dataset, output_dir: str):\n",
    "    \"\"\"\n",
    "    Trains the T5 model.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: The tokenized training dataset.\n",
    "        validation_dataset: The tokenized validation dataset.\n",
    "        output_dir (str): Directory to save the trained model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: Trained Trainer instance.\n",
    "    \"\"\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"T5 Training already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for T5...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training complete!\")\n",
    "\n",
    "    print(f\"Saving T5 model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"T5 Model and tokenizer saved.\")\n",
    "    \n",
    "    return trainer, model\n",
    "\n",
    "def compute_head_importance_and_prune_t5_model(model, trainer, validation_dataset):\n",
    "    \"\"\"\n",
    "    Computes head importance and prunes the T5 model.\n",
    "\n",
    "    Args:\n",
    "        model: The T5 model to prune.\n",
    "        trainer: The Trainer instance.\n",
    "        validation_dataset: The validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    head_importance = {layer: [0.0] * model.config.num_heads for layer in range(model.config.num_layers)}\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, output_attentions=True, return_dict=True)\n",
    "            encoder_attentions = outputs.encoder_attentions\n",
    "            decoder_attentions = outputs.decoder_attentions\n",
    "\n",
    "            for layer_idx in range(len(encoder_attentions)):\n",
    "                layer_mean = encoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_mean[head_idx].item()\n",
    "\n",
    "            for layer_idx in range(len(decoder_attentions)):\n",
    "                layer_dec_mean = decoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_dec_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_dec_mean[head_idx].item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        if total > 0:\n",
    "            head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "        else:\n",
    "            head_importance[layer] = [0.0] * model.config.num_heads\n",
    "\n",
    "    heads_to_prune = {layer: [head_idx for head_idx, score in enumerate(head_importance[layer]) if score < 0.05] for layer in head_importance}\n",
    "    \n",
    "    # Prune heads\n",
    "    for layer, heads in heads_to_prune.items():\n",
    "        if heads:\n",
    "            model.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "            model.decoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "\n",
    "def fine_tune_pruned_t5_model(model, train_dataset, validation_dataset):\n",
    "    \"\"\"\n",
    "    Fine-tunes the pruned T5 model.\n",
    "\n",
    "    Args:\n",
    "        model: The pruned T5 model.\n",
    "        train_dataset: The tokenized training dataset.\n",
    "        validation_dataset: The tokenized validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    trainer_pruned = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./results-pruned\",\n",
    "            num_train_epochs=50,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=os.path.join(\"./results-pruned\", \"logs\"),\n",
    "            logging_steps=10,\n",
    "            save_steps=10,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=10,\n",
    "            save_total_limit=1\n",
    "        ),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "    \n",
    "    eval_results = trainer_pruned.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"T5 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "def run_t5_model_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str):\n",
    "    \"\"\"\n",
    "    Executes the T5 model pipeline including data loading, preprocessing, training, head importance computation, pruning, and evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., \"cnn_dailymail\").\n",
    "        version (str): Version of the dataset to load.\n",
    "        sample_train_size (int): Number of training samples to use.\n",
    "        sample_val_size (int): Number of validation samples to use.\n",
    "        output_dir (str): Directory to save the trained model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints results and saves the model.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    train_data, validation_data = load_and_preprocess_t5_data(dataset_name, version, sample_train_size, sample_val_size)\n",
    "    train_dataset, validation_dataset = tokenize_t5_data(train_data, validation_data)\n",
    "    \n",
    "    # Train the T5 model\n",
    "    trainer, model = train_t5_model(train_dataset, validation_dataset, output_dir)\n",
    "\n",
    "    # Compute head importance and prune the T5 model\n",
    "    compute_head_importance_and_prune_t5_model(model, trainer, validation_dataset)\n",
    "\n",
    "    # Fine-tune the pruned T5 model\n",
    "    fine_tune_pruned_t5_model(model, train_dataset, validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
