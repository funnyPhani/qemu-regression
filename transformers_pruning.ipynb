{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 100, Validation size: 50\n",
      "Training for Initial Training already completed. Skipping...\n",
      "Saving model for Initial Training...\n",
      "Model and tokenizer saved for Initial Training.\n",
      "Warning: All heads in layer 0 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 1 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 2 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 3 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 4 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 5 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 6 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 7 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 8 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 9 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 10 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 11 are marked for pruning. Skipping pruning for this layer.\n",
      "Training for Automated Pruning already completed. Skipping...\n",
      "Saving model for Automated Pruning...\n",
      "Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98fb0f0e02641c0980f819621c9aefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.339203119277954, 'eval_runtime': 19.0693, 'eval_samples_per_second': 2.622, 'eval_steps_per_second': 0.682}\n",
      "Automated Pruning Perplexity: 28.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def tokenize_data(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def train_and_save_model(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def compute_head_importance(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "\n",
    "def prune_automated_heads(model: GPT2LMHeadModel, head_importance: Dict[int, float], threshold: float = 0.1):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head:\n",
    "            print(f\"Warning: All heads in layer {layer} are marked for pruning. Skipping pruning for this layer.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer to avoid division by zero\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary (to accommodate pruning)\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: GPT2LMHeadModel, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset\n",
    "    train_data, validation_data = load_and_preprocess_data(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    model = prune_automated_heads(model, head_importance, threshold=0.1)\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model(model, validation_dataset)\n",
    "    print(f\"Automated Pruning Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def load_pruned_model_and_tokenizer(output_dir: str):\n",
    "    # Load the pruned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 1):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions using beam search\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 num_beams=num_return_sequences,  # Use beam search\n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"./results-automated-pruning\"  # Directory where the pruned model is saved\n",
    "\n",
    "    # Load the pruned model and tokenizer\n",
    "    model, tokenizer = load_pruned_model_and_tokenizer(output_dir)\n",
    "\n",
    "    # Prepare input text for inference\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    \n",
    "    # Generate predictions\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 100, Validation size: 50\n",
      "Training for Initial Training already completed. Skipping...\n",
      "Saving model for Initial Training...\n",
      "Model and tokenizer saved for Initial Training.\n",
      "Layer 0 Head Importance Scores: [0.08333333361989413, 0.08333333323781307, 0.08333333323781307, 0.08333333285573201, 0.08333333323781307, 0.08333333400197519, 0.08333333361989413, 0.08333333400197519, 0.08333333400197519, 0.08333333247365095, 0.08333333361989413, 0.08333333209156989]\n",
      "Layer 1 Head Importance Scores: [0.08333333412933554, 0.08333333298309237, 0.08333333451141658, 0.08333333412933554, 0.0833333326010113, 0.0833333318368492, 0.08333333336517341, 0.08333333221893026, 0.08333333374725448, 0.08333333336517341, 0.08333333374725448, 0.08333333336517341]\n",
      "Layer 2 Head Importance Scores: [0.08333333336517341, 0.08333333336517341, 0.08333333298309237, 0.08333333298309237, 0.08333333298309237, 0.08333333221893026, 0.08333333069060603, 0.08333333527557871, 0.08333333298309237, 0.08333333527557871, 0.08333333298309237, 0.08333333489349765]\n",
      "Layer 3 Head Importance Scores: [0.08333333285573201, 0.08333333591238046, 0.08333333400197518, 0.08333333361989413, 0.08333333323781307, 0.08333333247365096, 0.08333333553029941, 0.08333333400197518, 0.08333333400197518, 0.08333333170948884, 0.08333333132740779, 0.08333333132740779]\n",
      "Layer 4 Head Importance Scores: [0.0833333326010113, 0.08333333489349766, 0.08333333145476814, 0.0833333345114166, 0.08333333107268708, 0.08333333183684918, 0.08333333489349766, 0.08333333298309237, 0.08333333336517341, 0.08333333374725448, 0.08333333527557871, 0.08333333336517341]\n",
      "Layer 5 Head Importance Scores: [0.08333333441589633, 0.08333333479797739, 0.08333333059508576, 0.08333333174132893, 0.08333333326965316, 0.08333333365173422, 0.08333333556213951, 0.08333333059508576, 0.08333333250549105, 0.08333333365173422, 0.08333333479797739, 0.08333333441589633]\n",
      "Layer 6 Head Importance Scores: [0.08333333285573201, 0.08333333170948883, 0.08333333553029942, 0.08333333361989413, 0.08333333438405624, 0.0833333301811646, 0.08333333056324566, 0.08333333323781307, 0.08333333629446155, 0.08333333170948883, 0.08333333514821836, 0.0833333347661373]\n",
      "Layer 7 Head Importance Scores: [0.08333333142292804, 0.08333333333333333, 0.08333333524373862, 0.0833333318050091, 0.08333333218709016, 0.08333333371541439, 0.08333333256917122, 0.08333333409749545, 0.08333333486165756, 0.08333333027668487, 0.08333333562581968, 0.08333333486165756]\n",
      "Layer 8 Head Importance Scores: [0.08333333377909458, 0.08333333568949987, 0.08333333377909458, 0.08333333186868927, 0.08333333377909458, 0.08333333377909458, 0.08333333454325668, 0.08333333339701351, 0.08333333301493245, 0.08333333339701351, 0.08333332957620292, 0.08333333339701351]\n",
      "Layer 9 Head Importance Scores: [0.08333333250549103, 0.0833333328875721, 0.08333332983092362, 0.08333333365173422, 0.0833333347979774, 0.08333333403381528, 0.08333333441589634, 0.08333333518005845, 0.08333333326965316, 0.0833333328875721, 0.08333333518005845, 0.08333333135924786]\n",
      "Layer 10 Head Importance Scores: [0.0833333336835743, 0.08333333406565537, 0.08333333444773643, 0.08333333444773643, 0.08333333482981749, 0.08333333291941218, 0.08333332871652054, 0.0833333336835743, 0.08333333253733112, 0.08333333597606066, 0.08333333062692583, 0.08333333406565537]\n",
      "Layer 11 Head Importance Scores: [0.08333333486165757, 0.08333333409749545, 0.08333333218709015, 0.08333333333333333, 0.08333333142292802, 0.0833333325691712, 0.0833333325691712, 0.08333333218709015, 0.08333333524373863, 0.08333333486165757, 0.08333333409749545, 0.0833333325691712]\n",
      "Training for Automated Pruning already completed. Skipping...\n",
      "Saving model for Automated Pruning...\n",
      "Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1d674fed9431bb23cf9baf65ce662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.339203119277954, 'eval_runtime': 18.2681, 'eval_samples_per_second': 2.737, 'eval_steps_per_second': 0.712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Pruning Perplexity: 28.20\n",
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def tokenize_data(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def train_and_save_model(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def compute_head_importance(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "\n",
    "def prune_automated_heads(model: GPT2LMHeadModel, head_importance: Dict[int, float], threshold: float = 0.2, min_heads_to_retain: int = 2):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - min_heads_to_retain:\n",
    "            print(f\"Warning: Not pruning layer {layer} to retain at least {min_heads_to_retain} heads.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer\n",
    "\n",
    "        print(f\"Layer {layer} Head Importance Scores: {importance_scores}\")\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: GPT2LMHeadModel, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def generate_text(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 3):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset\n",
    "    train_data, validation_data = load_and_preprocess_data(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    # model = prune_automated_heads(model, head_importance, threshold=0.2, min_heads_to_retain = 1)\n",
    "    model = prune_automated_heads(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model(model, validation_dataset)\n",
    "    print(f\"Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=200, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def load_pruned_model_and_tokenizer(output_dir: str):\n",
    "    # Load the pruned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 1):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions using beam search\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 num_beams=num_return_sequences,  # Use beam search\n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"./results-automated-pruning\"  # Directory where the pruned model is saved\n",
    "\n",
    "    # Load the pruned model and tokenizer\n",
    "    model, tokenizer = load_pruned_model_and_tokenizer(output_dir)\n",
    "\n",
    "    # Prepare input text for inference\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    \n",
    "    # Generate predictions\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations for Further Pruning\n",
    "# To enable more effective pruning, consider these options:\n",
    "\n",
    "# Lower the Threshold Further: Lowering the pruning threshold (currently set to 0.2) may help identify less important heads for pruning. You could try:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# model = prune_automated_heads(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "# Increase Training Dataset Size: Using a larger training dataset could help create more distinct importance scores, enabling better pruning decisions.\n",
    "\n",
    "# Manual Head Pruning Based on Scores: If automated pruning doesnâ€™t yield effective results, try manual pruning by removing the heads with the lowest importance scores across layers.\n",
    "\n",
    "# Different Metrics for Calculating Head Importance: You could explore other ways of calculating importance, such as using gradient-based metrics or fine-tuning loss-based metrics to identify heads for pruning.\n",
    "\n",
    "# Layer-wise Pruning: Instead of pruning heads from every layer, consider focusing on specific layers where pruning is most beneficial, such as middle or later layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3bee7a173a4f2cba8e7689251c181e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Training size: 100, Validation size: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ecd552396b44c1b28c78654093a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8100cc44d43f45858a0e988a7eacd512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for GPT-2 Initial Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0788b261ef641a7bd0b46dd9cd9bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6498, 'grad_norm': 12.507431030273438, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7337fe8cd6b434fbe68d428d5ba004c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3359274864196777, 'eval_runtime': 20.0587, 'eval_samples_per_second': 2.493, 'eval_steps_per_second': 0.648, 'epoch': 0.4}\n",
      "{'loss': 3.6782, 'grad_norm': 11.138589859008789, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2031071e724b4e80edc5af4404d1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3262970447540283, 'eval_runtime': 19.2813, 'eval_samples_per_second': 2.593, 'eval_steps_per_second': 0.674, 'epoch': 0.8}\n",
      "{'train_runtime': 172.6697, 'train_samples_per_second': 0.579, 'train_steps_per_second': 0.145, 'train_loss': 3.6711663818359375, 'epoch': 1.0}\n",
      "Training for Initial Training complete!\n",
      "Saving GPT-2 model for Initial Training...\n",
      "GPT-2 Model and tokenizer saved for Initial Training.\n",
      "Layer 0 Head Importance Scores: [0.08333333454325668, 0.08333333530741881, 0.08333333263285139, 0.08333333225077033, 0.08333333530741881, 0.08333333263285139, 0.08333333301493245, 0.08333333263285139, 0.08333333301493245, 0.08333333377909456, 0.08333333263285139, 0.08333333225077033]\n",
      "Layer 1 Head Importance Scores: [0.08333333333333333, 0.08333333524373862, 0.08333333256917122, 0.08333333409749545, 0.08333333371541439, 0.08333333371541439, 0.08333333409749545, 0.0833333318050091, 0.08333333409749545, 0.08333333409749545, 0.08333333218709016, 0.08333333104084699]\n",
      "Layer 2 Head Importance Scores: [0.08333333530741882, 0.08333333339701351, 0.08333333339701351, 0.08333333339701351, 0.0833333314866082, 0.08333333339701351, 0.08333333377909458, 0.08333333263285139, 0.08333333263285139, 0.0833333345432567, 0.08333333377909458, 0.08333333225077033]\n",
      "Layer 3 Head Importance Scores: [0.08333333412933554, 0.08333333298309237, 0.08333333221893025, 0.08333333489349766, 0.08333333374725448, 0.08333333336517341, 0.08333333221893025, 0.08333333336517341, 0.08333333298309237, 0.08333333412933554, 0.08333333221893025, 0.08333333374725448]\n",
      "Layer 4 Head Importance Scores: [0.08333333454325668, 0.08333333225077033, 0.08333333416117562, 0.08333333530741881, 0.08333333263285139, 0.08333333301493245, 0.08333333377909458, 0.08333333377909458, 0.08333333339701351, 0.08333333339701351, 0.08333333110452715, 0.08333333263285139]\n",
      "Layer 5 Head Importance Scores: [0.08333333269653158, 0.08333333460693686, 0.08333333307861263, 0.08333333269653158, 0.08333333460693686, 0.0833333342248558, 0.08333333155028841, 0.08333333040404524, 0.0833333342248558, 0.08333333537109897, 0.08333333460693686, 0.08333333193236947]\n",
      "Layer 6 Head Importance Scores: [0.08333333174132894, 0.0833333328875721, 0.08333333403381527, 0.08333333174132894, 0.08333333097716682, 0.08333333441589633, 0.08333333403381527, 0.08333333135924788, 0.08333333250549105, 0.0833333355621395, 0.0833333355621395, 0.08333333518005843]\n",
      "Layer 7 Head Importance Scores: [0.08333333463877693, 0.08333333387461482, 0.08333333272837166, 0.08333333349253377, 0.08333332928964218, 0.08333333196420956, 0.08333333502085799, 0.08333333349253377, 0.08333333502085799, 0.08333333463877693, 0.08333333463877693, 0.08333333120004745]\n",
      "Layer 8 Head Importance Scores: [0.08333333247365095, 0.08333333170948883, 0.08333333400197519, 0.08333333323781307, 0.08333333247365095, 0.0833333347661373, 0.08333333247365095, 0.08333333667654261, 0.08333333132740778, 0.08333333553029942, 0.08333333400197519, 0.08333333132740778]\n",
      "Layer 9 Head Importance Scores: [0.08333333320597298, 0.08333333740886463, 0.08333333435221615, 0.08333333129556768, 0.08333333320597298, 0.08333333244181086, 0.08333333358805405, 0.08333333167764874, 0.08333333435221615, 0.08333333244181086, 0.08333333282389192, 0.08333333320597298]\n",
      "Layer 10 Head Importance Scores: [0.08333333279205184, 0.08333333202788971, 0.0833333331741329, 0.08333333240997078, 0.08333333432037607, 0.08333333623078136, 0.08333333508453819, 0.08333333508453819, 0.0833333331741329, 0.0833333331741329, 0.08333333279205184, 0.08333332973540336]\n",
      "Layer 11 Head Importance Scores: [0.08333333508453818, 0.08333333470245712, 0.0833333331741329, 0.08333333470245712, 0.08333333240997078, 0.0833333358487003, 0.08333333164580867, 0.08333333164580867, 0.08333333279205184, 0.083333333938295, 0.0833333331741329, 0.08333333088164654]\n",
      "GPT-2 Training for Automated Pruning already completed. Skipping...\n",
      "Saving GPT-2 model for Automated Pruning...\n",
      "GPT-2 Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f17d54983804f93bf06ffaae6599cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.3204500675201416, 'eval_runtime': 18.8328, 'eval_samples_per_second': 2.655, 'eval_steps_per_second': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Automated Pruning Perplexity: 27.67\n",
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a new book by Dr. David S. Siegel, PhD, a professor of medicine at the University of California, San Francisco, and a co-author of the new paper. It is available from Amazon.com. (2) Drs.Siegel and Sussman are coauthors of a paper in the journal Science Advances. They are also coauthoring the paper with Dr David A. Schoenfeld, MD, of Harvard Medical School. Dr Skelton is also a member of The American Academy of Pediatrics.\n"
     ]
    }
   ],
   "source": [
    "#  GPT-2 Model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and preprocess data function for GPT-2\n",
    "def load_and_preprocess_data_gpt2(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"GPT-2 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "# Tokenization function for GPT-2\n",
    "def tokenize_data_gpt2(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "# Training function for GPT-2\n",
    "def train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"GPT-2 Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for GPT-2 {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving GPT-2 model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"GPT-2 Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "# Head importance computation for GPT-2\n",
    "def compute_head_importance_gpt2(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "# Pruning function for GPT-2\n",
    "def prune_automated_heads_gpt2(model, head_importance: Dict[int, float], threshold: float = 0.2, min_heads_to_retain: int = 2):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - min_heads_to_retain:\n",
    "            print(f\"Warning: Not pruning layer {layer} to retain at least {min_heads_to_retain} heads.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer\n",
    "\n",
    "        print(f\"Layer {layer} Head Importance Scores: {importance_scores}\")\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluation function for GPT-2\n",
    "def evaluate_model_gpt2(model, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "# Text generation function for GPT-2\n",
    "def generate_text_gpt2(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 3):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Main execution for GPT-2\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset for GPT-2\n",
    "    train_data, validation_data = load_and_preprocess_data_gpt2(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data_gpt2(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-gpt2-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance_gpt2(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    model = prune_automated_heads_gpt2(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model_gpt2(model, validation_dataset)\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    generated_texts = generate_text_gpt2(model, tokenizer, input_text, max_length=200, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Model\n",
    "\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "from functools import partial\n",
    "\n",
    "# Set parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 4\n",
    "LR = 0.00005\n",
    "EPOCHS = 1\n",
    "MODEL = 'roberta-base'  # Change to 'roberta-base'\n",
    "OUT_DIR = 'arxiv_roberta'  # Update the output directory\n",
    "\n",
    "# Load datasets (using a sample for demonstration)\n",
    "train_dataset = load_dataset(\"ccdv/arxiv-classification\", split='train[:5%]')\n",
    "valid_dataset = load_dataset(\"ccdv/arxiv-classification\", split='validation[:10%]')\n",
    "test_dataset = load_dataset(\"ccdv/arxiv-classification\", split='test[:10%]')\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "            5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess_function(tokenizer, examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "tokenized_valid = valid_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "tokenized_test = test_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Metric computation\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Model training setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=11, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to='tensorboard',\n",
    "    # Remove the following line\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Pruning function for dynamic pruning\n",
    "def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "    \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "    for step in range(steps):\n",
    "        print(f\"Pruning step {step + 1}/{steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        # Fine-tune the model after pruning\n",
    "        trainer.train()\n",
    "        # Remove pruning and make it permanent\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "# Apply dynamic pruning\n",
    "dynamic_prune_model(model, amount=0.1, steps=3)\n",
    "\n",
    "# Save the pruned model\n",
    "model.save_pretrained('pruned_arxiv_roberta')\n",
    "tokenizer.save_pretrained('pruned_arxiv_roberta')\n",
    "\n",
    "# Load the pruned model for inference\n",
    "model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_roberta')\n",
    "tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_roberta')\n",
    "classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Evaluate the pruned model on the test dataset\n",
    "eval_result = trainer.evaluate(tokenized_test)\n",
    "print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "# Inference with the pruned model\n",
    "all_files = glob.glob('inference_data/*')\n",
    "for file_name in all_files:\n",
    "    with open(file_name) as file:\n",
    "        content = file.read()\n",
    "        result = classify(content)\n",
    "        print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=1,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and prune the ALBERT model for text classification on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to use.\n",
    "        model_name (str): Pre-trained model name.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "        pruning_amount (float): Fraction of weights to prune in each step.\n",
    "        pruning_steps (int): Number of pruning steps.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split='train[:95%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split='validation[:100%]')\n",
    "    test_dataset = load_dataset(dataset_name, split='test[:100%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=11, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_albert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_albert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                content = file.read()\n",
    "                result = classify(content)\n",
    "                print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Call the function with custom arguments\n",
    "run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=50,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DistilBERT Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def run_distilbert_pipeline(dataset_name=\"ccdv/arxiv-classification\", \n",
    "                            train_split='train[:95%]', \n",
    "                            valid_split='validation[:100%]', \n",
    "                            test_split='test[:100%]', \n",
    "                            model_name='distilbert-base-uncased', \n",
    "                            batch_size=32, \n",
    "                            num_procs=4, \n",
    "                            lr=0.00005, \n",
    "                            epochs=1, \n",
    "                            prune_amount=0.1, \n",
    "                            prune_steps=3, \n",
    "                            out_dir='arxiv_distilbert'):\n",
    "    \"\"\"\n",
    "    Executes the DistilBERT model pipeline including dataset loading, tokenization,\n",
    "    model training, dynamic pruning, evaluation, and inference.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        test_split (str): Slice of the test data.\n",
    "        model_name (str): Name of the pretrained DistilBERT model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        prune_amount (float): Fraction of weights to prune.\n",
    "        prune_steps (int): Number of pruning steps.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    valid_dataset = load_dataset(dataset_name, split=valid_split)\n",
    "    test_dataset = load_dataset(dataset_name, split=test_split)\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Pruning function for dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=prune_amount, steps=prune_steps):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_distilbert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_distilbert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Call the function with specified arguments\n",
    "  run_distilbert_pipeline(\n",
    "      dataset_name=\"ccdv/arxiv-classification\", \n",
    "      train_split='train[:95%]', \n",
    "      valid_split='validation[:85%]', \n",
    "      test_split='test[:85%]', \n",
    "      model_name='distilbert-base-uncased', \n",
    "      batch_size=32, \n",
    "      num_procs=4, \n",
    "      lr=0.00005, \n",
    "      epochs=50, \n",
    "      prune_amount=0.1, \n",
    "      prune_steps=3, \n",
    "      out_dir='arxiv_distilbert'\n",
    ")\n",
    "\n",
    "# T5 Model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def run_t5_model_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str):\n",
    "    \"\"\"\n",
    "    Executes the T5 model pipeline including data loading, preprocessing, training, head importance computation, pruning, and evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., \"cnn_dailymail\").\n",
    "        version (str): Version of the dataset to load.\n",
    "        sample_train_size (int): Number of training samples to use.\n",
    "        sample_val_size (int): Number of validation samples to use.\n",
    "        output_dir (str): Directory to save the trained model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints results and saves the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"input_text\": x['article'], \"target_text\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"T5 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(examples['input_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        labels = tokenizer(examples['target_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "        return inputs\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Training\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"T5 Training already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for T5...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training complete!\")\n",
    "\n",
    "    print(f\"Saving T5 model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"T5 Model and tokenizer saved.\")\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {layer: [0.0] * model.config.num_heads for layer in range(model.config.num_layers)}\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, output_attentions=True, return_dict=True)\n",
    "            encoder_attentions = outputs.encoder_attentions\n",
    "            decoder_attentions = outputs.decoder_attentions\n",
    "\n",
    "            for layer_idx in range(len(encoder_attentions)):\n",
    "                layer_mean = encoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_mean[head_idx].item()\n",
    "\n",
    "            for layer_idx in range(len(decoder_attentions)):\n",
    "                layer_dec_mean = decoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_dec_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_dec_mean[head_idx].item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        if total > 0:\n",
    "            head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "        else:\n",
    "            head_importance[layer] = [0.0] * model.config.num_heads\n",
    "\n",
    "    heads_to_prune = {layer: [head_idx for head_idx, score in enumerate(head_importance[layer]) if score < 0.05] for layer in head_importance}\n",
    "    \n",
    "    # Prune heads\n",
    "    for layer, heads in heads_to_prune.items():\n",
    "        if heads:\n",
    "            model.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "            model.decoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "\n",
    "    model.eval()\n",
    "    trainer_pruned = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer_pruned.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"T5 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_t5_model_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-t5-automated-pruning\")\n",
    "\n",
    "\n",
    "\n",
    "# RoBERTa Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def roberta_pipeline(model_name: str, \n",
    "                    dataset_name: str, \n",
    "                    sample_fraction: float, \n",
    "                    out_dir: str, \n",
    "                    batch_size: int = 16, \n",
    "                    num_epochs: int = 5, \n",
    "                    learning_rate: float = 5e-5,\n",
    "                    pruning_amount: float = 0.1, \n",
    "                    pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete RoBERTa pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained RoBERTa model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "# Main execution for RoBERTa\n",
    "if __name__ == \"__main__\":\n",
    "    roberta_pipeline(\n",
    "        model_name='roberta-base',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.95,  # Use 95% of training data\n",
    "        out_dir='arxiv_roberta',\n",
    "        batch_size=16,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "\n",
    "\n",
    "# BERT Model\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def bert_pipeline(model_name: str, \n",
    "                 dataset_name: str, \n",
    "                 sample_fraction: float, \n",
    "                 out_dir: str, \n",
    "                 batch_size: int = 32, \n",
    "                 num_epochs: int = 3, \n",
    "                 learning_rate: float = 5e-5,\n",
    "                 pruning_amount: float = 0.1, \n",
    "                 pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete BERT pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained BERT model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "# Main execution for BERT\n",
    "if __name__ == \"__main__\":\n",
    "    bert_pipeline(\n",
    "        model_name='bert-base-uncased',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.8,\n",
    "        out_dir='arxiv_bert',\n",
    "        batch_size=32,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "\n",
    "\n",
    "# GPT2 Model\n",
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def gpt2_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str, input_text: str):\n",
    "    \"\"\"Complete GPT-2 pipeline including loading data, training, pruning, evaluation, and text generation.\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    \n",
    "    # Load tokenizer and set padding token\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_data(data):\n",
    "        def tokenize_function(examples):\n",
    "            tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "            tokens['labels'] = tokens['input_ids'].copy()\n",
    "            return tokens\n",
    "\n",
    "        return data.map(tokenize_function, batched=True).set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    train_dataset = tokenize_data(train_data)\n",
    "    validation_dataset = tokenize_data(validation_data)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        head_importance[layer] = [0.0] * model.config.n_head\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    # Prune heads based on importance\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < 0.2]\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - 1:\n",
    "            heads_to_prune[layer] = []  # Skip pruning if too many heads would be removed\n",
    "\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Text generation\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Main execution for GPT-2\n",
    "if __name__ == \"__main__\":\n",
    "    generated = gpt2_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-gpt2-automated-pruning\", \"The future of AI in healthcare is\")\n",
    "    for i, text in enumerate(generated):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2 model: Perplexity: 25.96\n",
    "\n",
    "BERT Model: Accuracy: 85.95\n",
    "RoBERTa Model: Accuracy: 81.69\n",
    "T5Model: Perplexity: 155.25\n",
    "DistilBERT Model: Accuracy: 89.25\n",
    "ALBERT Model: Accuracy: 75.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Results\n",
    "\n",
    "This repository contains the implementation and evaluation of various transformer models for natural language processing tasks. The primary models evaluated include GPT-2, BERT, RoBERTa, T5, DistilBERT, and ALBERT.\n",
    "\n",
    "## Models and Evaluation Metrics\n",
    "\n",
    "The following models have been evaluated with their respective metrics:\n",
    "\n",
    "| Model                | Evaluation Accuracy | Perplexity | Model Size After Pruning |\n",
    "|----------------------|---------------------|------------|--------------------------|\n",
    "| ALBERT               | 75.69%              | -          | 11 MB                    |\n",
    "| BERT                 | 85.95%              | -          | 420 MB                   |\n",
    "| DistilBERT           | 89.25%              | -          | 66 MB                    |\n",
    "| RoBERTa              | 81.69%              | -          | 500 MB                   |\n",
    "| GPT-2                | -                   | 25.96      | 345 MB                   |\n",
    "| T5                   | -                   | 155.25     | 220 MB                   |\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To run the models in this repository, you will need the following Python packages:\n",
    "\n",
    "- `torch`\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `evaluate`\n",
    "- `numpy`\n",
    "\n",
    "You can install the required packages using pip:\n",
    "\n",
    "```bash\n",
    "pip install torch transformers datasets evaluate numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Unsloth for fine-tuning the LLM-transformer model\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "def fine_tune_llm_qwen2_5_or_llama3_or_phi3_5(model_name = \"unsloth/Qwen2.5-0.5B\",dataset_name = \"mlabonne/FineTome-100k\",epochs = 5):\n",
    "    try:\n",
    "        if \"Qwen2\" in model_name:\n",
    "            chat_template = \"qwen-2.5\"\n",
    "        elif \"Llama-3.2\" in model_name:\n",
    "            chat_template = \"llama-3.1\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            chat_template = \"llama-3.1\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            chat_template = \"phi-3\"\n",
    "\n",
    "\n",
    "        from unsloth import FastLanguageModel\n",
    "        import torch\n",
    "        max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "        dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "        load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "        # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "        fourbit_models = [\n",
    "            \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "            \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "            \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "            \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "            \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "            \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "            \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "            \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "            \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "            \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "            \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "            \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "        ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            # Can select any from the below:\n",
    "            # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "            # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "            # And also all Instruct versions and Math. Coding verisons!\n",
    "            model_name = model_name,\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "            # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "        )\n",
    "\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "                    model,\n",
    "                    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "                    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "                    lora_alpha = 16,\n",
    "                    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "                    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "                    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "                    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "                    random_state = 3407,\n",
    "                    use_rslora = False,  # We support rank stabilized LoRA\n",
    "                    loftq_config = None, # And LoftQ\n",
    "                )\n",
    "\n",
    "\n",
    "        def formatting_prompts_func(examples):\n",
    "            convos = examples[\"conversations\"]\n",
    "            texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "            return { \"text\" : texts, }\n",
    "        pass\n",
    "\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(dataset_name, split = \"train\")\n",
    "        from unsloth.chat_templates import standardize_sharegpt\n",
    "        dataset = standardize_sharegpt(dataset)\n",
    "        dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "        from rich import print\n",
    "        print(dataset)\n",
    "        print(dataset[5][\"conversations\"])\n",
    "        print(dataset[5][\"text\"])\n",
    "        from trl import SFTTrainer\n",
    "        from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "        from unsloth import is_bfloat16_supported\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            train_dataset = dataset,\n",
    "            dataset_text_field = \"text\",\n",
    "            max_seq_length = max_seq_length,\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "            dataset_num_proc = 2,\n",
    "            packing = False, # Can make training 5x faster for short sequences.\n",
    "            args = TrainingArguments(\n",
    "                per_device_train_batch_size = 2,\n",
    "                gradient_accumulation_steps = 4,\n",
    "                warmup_steps = epochs,\n",
    "                num_train_epochs = 10, # Set this for 1 full training run.\n",
    "                # max_steps = 60,\n",
    "                learning_rate = 2e-4,\n",
    "                fp16 = not is_bfloat16_supported(),\n",
    "                bf16 = is_bfloat16_supported(),\n",
    "                logging_steps = 1,\n",
    "                optim = \"adamw_8bit\",\n",
    "                weight_decay = 0.01,\n",
    "                lr_scheduler_type = \"linear\",\n",
    "                seed = 3407,\n",
    "                output_dir = \"outputs\",\n",
    "            ),\n",
    "        )\n",
    "        from unsloth.chat_templates import train_on_responses_only\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|im_start|>user\\n\",\n",
    "            response_part = \"<|im_start|>assistant\\n\",\n",
    "        )\n",
    "        print(tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]))\n",
    "        space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "        print(tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]))\n",
    "        #@title Show current memory stats\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "        trainer_stats = trainer.train()\n",
    "        #@title Show final memory and time stats\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "        from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template = chat_template,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                                temperature = 1.5, min_p = 0.1)\n",
    "        print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        from transformers import TextStreamer\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "        _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                        use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "        \n",
    "        model.save_pretrained(\"lora_model\") # Local saving\n",
    "        tokenizer.save_pretrained(\"lora_model\")\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
