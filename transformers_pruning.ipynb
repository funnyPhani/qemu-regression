{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 100, Validation size: 50\n",
      "Training for Initial Training already completed. Skipping...\n",
      "Saving model for Initial Training...\n",
      "Model and tokenizer saved for Initial Training.\n",
      "Warning: All heads in layer 0 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 1 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 2 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 3 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 4 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 5 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 6 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 7 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 8 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 9 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 10 are marked for pruning. Skipping pruning for this layer.\n",
      "Warning: All heads in layer 11 are marked for pruning. Skipping pruning for this layer.\n",
      "Training for Automated Pruning already completed. Skipping...\n",
      "Saving model for Automated Pruning...\n",
      "Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98fb0f0e02641c0980f819621c9aefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.339203119277954, 'eval_runtime': 19.0693, 'eval_samples_per_second': 2.622, 'eval_steps_per_second': 0.682}\n",
      "Automated Pruning Perplexity: 28.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def tokenize_data(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def train_and_save_model(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def compute_head_importance(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "\n",
    "def prune_automated_heads(model: GPT2LMHeadModel, head_importance: Dict[int, float], threshold: float = 0.1):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head:\n",
    "            print(f\"Warning: All heads in layer {layer} are marked for pruning. Skipping pruning for this layer.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer to avoid division by zero\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary (to accommodate pruning)\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: GPT2LMHeadModel, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset\n",
    "    train_data, validation_data = load_and_preprocess_data(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    model = prune_automated_heads(model, head_importance, threshold=0.1)\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model(model, validation_dataset)\n",
    "    print(f\"Automated Pruning Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def load_pruned_model_and_tokenizer(output_dir: str):\n",
    "    # Load the pruned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 1):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions using beam search\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 num_beams=num_return_sequences,  # Use beam search\n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"./results-automated-pruning\"  # Directory where the pruned model is saved\n",
    "\n",
    "    # Load the pruned model and tokenizer\n",
    "    model, tokenizer = load_pruned_model_and_tokenizer(output_dir)\n",
    "\n",
    "    # Prepare input text for inference\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    \n",
    "    # Generate predictions\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 100, Validation size: 50\n",
      "Training for Initial Training already completed. Skipping...\n",
      "Saving model for Initial Training...\n",
      "Model and tokenizer saved for Initial Training.\n",
      "Layer 0 Head Importance Scores: [0.08333333361989413, 0.08333333323781307, 0.08333333323781307, 0.08333333285573201, 0.08333333323781307, 0.08333333400197519, 0.08333333361989413, 0.08333333400197519, 0.08333333400197519, 0.08333333247365095, 0.08333333361989413, 0.08333333209156989]\n",
      "Layer 1 Head Importance Scores: [0.08333333412933554, 0.08333333298309237, 0.08333333451141658, 0.08333333412933554, 0.0833333326010113, 0.0833333318368492, 0.08333333336517341, 0.08333333221893026, 0.08333333374725448, 0.08333333336517341, 0.08333333374725448, 0.08333333336517341]\n",
      "Layer 2 Head Importance Scores: [0.08333333336517341, 0.08333333336517341, 0.08333333298309237, 0.08333333298309237, 0.08333333298309237, 0.08333333221893026, 0.08333333069060603, 0.08333333527557871, 0.08333333298309237, 0.08333333527557871, 0.08333333298309237, 0.08333333489349765]\n",
      "Layer 3 Head Importance Scores: [0.08333333285573201, 0.08333333591238046, 0.08333333400197518, 0.08333333361989413, 0.08333333323781307, 0.08333333247365096, 0.08333333553029941, 0.08333333400197518, 0.08333333400197518, 0.08333333170948884, 0.08333333132740779, 0.08333333132740779]\n",
      "Layer 4 Head Importance Scores: [0.0833333326010113, 0.08333333489349766, 0.08333333145476814, 0.0833333345114166, 0.08333333107268708, 0.08333333183684918, 0.08333333489349766, 0.08333333298309237, 0.08333333336517341, 0.08333333374725448, 0.08333333527557871, 0.08333333336517341]\n",
      "Layer 5 Head Importance Scores: [0.08333333441589633, 0.08333333479797739, 0.08333333059508576, 0.08333333174132893, 0.08333333326965316, 0.08333333365173422, 0.08333333556213951, 0.08333333059508576, 0.08333333250549105, 0.08333333365173422, 0.08333333479797739, 0.08333333441589633]\n",
      "Layer 6 Head Importance Scores: [0.08333333285573201, 0.08333333170948883, 0.08333333553029942, 0.08333333361989413, 0.08333333438405624, 0.0833333301811646, 0.08333333056324566, 0.08333333323781307, 0.08333333629446155, 0.08333333170948883, 0.08333333514821836, 0.0833333347661373]\n",
      "Layer 7 Head Importance Scores: [0.08333333142292804, 0.08333333333333333, 0.08333333524373862, 0.0833333318050091, 0.08333333218709016, 0.08333333371541439, 0.08333333256917122, 0.08333333409749545, 0.08333333486165756, 0.08333333027668487, 0.08333333562581968, 0.08333333486165756]\n",
      "Layer 8 Head Importance Scores: [0.08333333377909458, 0.08333333568949987, 0.08333333377909458, 0.08333333186868927, 0.08333333377909458, 0.08333333377909458, 0.08333333454325668, 0.08333333339701351, 0.08333333301493245, 0.08333333339701351, 0.08333332957620292, 0.08333333339701351]\n",
      "Layer 9 Head Importance Scores: [0.08333333250549103, 0.0833333328875721, 0.08333332983092362, 0.08333333365173422, 0.0833333347979774, 0.08333333403381528, 0.08333333441589634, 0.08333333518005845, 0.08333333326965316, 0.0833333328875721, 0.08333333518005845, 0.08333333135924786]\n",
      "Layer 10 Head Importance Scores: [0.0833333336835743, 0.08333333406565537, 0.08333333444773643, 0.08333333444773643, 0.08333333482981749, 0.08333333291941218, 0.08333332871652054, 0.0833333336835743, 0.08333333253733112, 0.08333333597606066, 0.08333333062692583, 0.08333333406565537]\n",
      "Layer 11 Head Importance Scores: [0.08333333486165757, 0.08333333409749545, 0.08333333218709015, 0.08333333333333333, 0.08333333142292802, 0.0833333325691712, 0.0833333325691712, 0.08333333218709015, 0.08333333524373863, 0.08333333486165757, 0.08333333409749545, 0.0833333325691712]\n",
      "Training for Automated Pruning already completed. Skipping...\n",
      "Saving model for Automated Pruning...\n",
      "Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1d674fed9431bb23cf9baf65ce662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.339203119277954, 'eval_runtime': 18.2681, 'eval_samples_per_second': 2.737, 'eval_steps_per_second': 0.712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Pruning Perplexity: 28.20\n",
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def tokenize_data(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def train_and_save_model(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def compute_head_importance(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "\n",
    "def prune_automated_heads(model: GPT2LMHeadModel, head_importance: Dict[int, float], threshold: float = 0.2, min_heads_to_retain: int = 2):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - min_heads_to_retain:\n",
    "            print(f\"Warning: Not pruning layer {layer} to retain at least {min_heads_to_retain} heads.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer\n",
    "\n",
    "        print(f\"Layer {layer} Head Importance Scores: {importance_scores}\")\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: GPT2LMHeadModel, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def generate_text(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 3):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset\n",
    "    train_data, validation_data = load_and_preprocess_data(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    # model = prune_automated_heads(model, head_importance, threshold=0.2, min_heads_to_retain = 1)\n",
    "    model = prune_automated_heads(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model(model, validation_dataset)\n",
    "    print(f\"Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=200, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a book that explores the future and the challenges facing healthcare. It is an exploration of the ways in which healthcare will change the way we live, work, and play. This book is intended to be a resource for those who are interested in the development of healthcare and how it will affect their lives.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def load_pruned_model_and_tokenizer(output_dir: str):\n",
    "    # Load the pruned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 1):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate predictions using beam search\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 num_beams=num_return_sequences,  # Use beam search\n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"./results-automated-pruning\"  # Directory where the pruned model is saved\n",
    "\n",
    "    # Load the pruned model and tokenizer\n",
    "    model, tokenizer = load_pruned_model_and_tokenizer(output_dir)\n",
    "\n",
    "    # Prepare input text for inference\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    \n",
    "    # Generate predictions\n",
    "    generated_texts = generate_text(model, tokenizer, input_text, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations for Further Pruning\n",
    "# To enable more effective pruning, consider these options:\n",
    "\n",
    "# Lower the Threshold Further: Lowering the pruning threshold (currently set to 0.2) may help identify less important heads for pruning. You could try:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# model = prune_automated_heads(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "# Increase Training Dataset Size: Using a larger training dataset could help create more distinct importance scores, enabling better pruning decisions.\n",
    "\n",
    "# Manual Head Pruning Based on Scores: If automated pruning doesnâ€™t yield effective results, try manual pruning by removing the heads with the lowest importance scores across layers.\n",
    "\n",
    "# Different Metrics for Calculating Head Importance: You could explore other ways of calculating importance, such as using gradient-based metrics or fine-tuning loss-based metrics to identify heads for pruning.\n",
    "\n",
    "# Layer-wise Pruning: Instead of pruning heads from every layer, consider focusing on specific layers where pruning is most beneficial, such as middle or later layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3bee7a173a4f2cba8e7689251c181e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Training size: 100, Validation size: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ecd552396b44c1b28c78654093a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8100cc44d43f45858a0e988a7eacd512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for GPT-2 Initial Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0788b261ef641a7bd0b46dd9cd9bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6498, 'grad_norm': 12.507431030273438, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7337fe8cd6b434fbe68d428d5ba004c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3359274864196777, 'eval_runtime': 20.0587, 'eval_samples_per_second': 2.493, 'eval_steps_per_second': 0.648, 'epoch': 0.4}\n",
      "{'loss': 3.6782, 'grad_norm': 11.138589859008789, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2031071e724b4e80edc5af4404d1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3262970447540283, 'eval_runtime': 19.2813, 'eval_samples_per_second': 2.593, 'eval_steps_per_second': 0.674, 'epoch': 0.8}\n",
      "{'train_runtime': 172.6697, 'train_samples_per_second': 0.579, 'train_steps_per_second': 0.145, 'train_loss': 3.6711663818359375, 'epoch': 1.0}\n",
      "Training for Initial Training complete!\n",
      "Saving GPT-2 model for Initial Training...\n",
      "GPT-2 Model and tokenizer saved for Initial Training.\n",
      "Layer 0 Head Importance Scores: [0.08333333454325668, 0.08333333530741881, 0.08333333263285139, 0.08333333225077033, 0.08333333530741881, 0.08333333263285139, 0.08333333301493245, 0.08333333263285139, 0.08333333301493245, 0.08333333377909456, 0.08333333263285139, 0.08333333225077033]\n",
      "Layer 1 Head Importance Scores: [0.08333333333333333, 0.08333333524373862, 0.08333333256917122, 0.08333333409749545, 0.08333333371541439, 0.08333333371541439, 0.08333333409749545, 0.0833333318050091, 0.08333333409749545, 0.08333333409749545, 0.08333333218709016, 0.08333333104084699]\n",
      "Layer 2 Head Importance Scores: [0.08333333530741882, 0.08333333339701351, 0.08333333339701351, 0.08333333339701351, 0.0833333314866082, 0.08333333339701351, 0.08333333377909458, 0.08333333263285139, 0.08333333263285139, 0.0833333345432567, 0.08333333377909458, 0.08333333225077033]\n",
      "Layer 3 Head Importance Scores: [0.08333333412933554, 0.08333333298309237, 0.08333333221893025, 0.08333333489349766, 0.08333333374725448, 0.08333333336517341, 0.08333333221893025, 0.08333333336517341, 0.08333333298309237, 0.08333333412933554, 0.08333333221893025, 0.08333333374725448]\n",
      "Layer 4 Head Importance Scores: [0.08333333454325668, 0.08333333225077033, 0.08333333416117562, 0.08333333530741881, 0.08333333263285139, 0.08333333301493245, 0.08333333377909458, 0.08333333377909458, 0.08333333339701351, 0.08333333339701351, 0.08333333110452715, 0.08333333263285139]\n",
      "Layer 5 Head Importance Scores: [0.08333333269653158, 0.08333333460693686, 0.08333333307861263, 0.08333333269653158, 0.08333333460693686, 0.0833333342248558, 0.08333333155028841, 0.08333333040404524, 0.0833333342248558, 0.08333333537109897, 0.08333333460693686, 0.08333333193236947]\n",
      "Layer 6 Head Importance Scores: [0.08333333174132894, 0.0833333328875721, 0.08333333403381527, 0.08333333174132894, 0.08333333097716682, 0.08333333441589633, 0.08333333403381527, 0.08333333135924788, 0.08333333250549105, 0.0833333355621395, 0.0833333355621395, 0.08333333518005843]\n",
      "Layer 7 Head Importance Scores: [0.08333333463877693, 0.08333333387461482, 0.08333333272837166, 0.08333333349253377, 0.08333332928964218, 0.08333333196420956, 0.08333333502085799, 0.08333333349253377, 0.08333333502085799, 0.08333333463877693, 0.08333333463877693, 0.08333333120004745]\n",
      "Layer 8 Head Importance Scores: [0.08333333247365095, 0.08333333170948883, 0.08333333400197519, 0.08333333323781307, 0.08333333247365095, 0.0833333347661373, 0.08333333247365095, 0.08333333667654261, 0.08333333132740778, 0.08333333553029942, 0.08333333400197519, 0.08333333132740778]\n",
      "Layer 9 Head Importance Scores: [0.08333333320597298, 0.08333333740886463, 0.08333333435221615, 0.08333333129556768, 0.08333333320597298, 0.08333333244181086, 0.08333333358805405, 0.08333333167764874, 0.08333333435221615, 0.08333333244181086, 0.08333333282389192, 0.08333333320597298]\n",
      "Layer 10 Head Importance Scores: [0.08333333279205184, 0.08333333202788971, 0.0833333331741329, 0.08333333240997078, 0.08333333432037607, 0.08333333623078136, 0.08333333508453819, 0.08333333508453819, 0.0833333331741329, 0.0833333331741329, 0.08333333279205184, 0.08333332973540336]\n",
      "Layer 11 Head Importance Scores: [0.08333333508453818, 0.08333333470245712, 0.0833333331741329, 0.08333333470245712, 0.08333333240997078, 0.0833333358487003, 0.08333333164580867, 0.08333333164580867, 0.08333333279205184, 0.083333333938295, 0.0833333331741329, 0.08333333088164654]\n",
      "GPT-2 Training for Automated Pruning already completed. Skipping...\n",
      "Saving GPT-2 model for Automated Pruning...\n",
      "GPT-2 Model and tokenizer saved for Automated Pruning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f17d54983804f93bf06ffaae6599cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 3.3204500675201416, 'eval_runtime': 18.8328, 'eval_samples_per_second': 2.655, 'eval_steps_per_second': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Automated Pruning Perplexity: 27.67\n",
      "Generated Text 1: The future of AI in healthcare is uncertain. The future is not yet clear.\n",
      "\n",
      "The Future of Healthcare\n",
      "...\n",
      ",\n",
      " (1) The Future Of Healthcare is a new book by Dr. David S. Siegel, PhD, a professor of medicine at the University of California, San Francisco, and a co-author of the new paper. It is available from Amazon.com. (2) Drs.Siegel and Sussman are coauthors of a paper in the journal Science Advances. They are also coauthoring the paper with Dr David A. Schoenfeld, MD, of Harvard Medical School. Dr Skelton is also a member of The American Academy of Pediatrics.\n"
     ]
    }
   ],
   "source": [
    "#  GPT-2 Model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and preprocess data function for GPT-2\n",
    "def load_and_preprocess_data_gpt2(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int):\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"GPT-2 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "    return train_data, validation_data\n",
    "\n",
    "# Tokenization function for GPT-2\n",
    "def tokenize_data_gpt2(train_data, validation_data, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        tokens['labels'] = tokens['input_ids'].copy()\n",
    "        return tokens\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "# Training function for GPT-2\n",
    "def train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir: str, phase: str):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"GPT-2 Training for {phase} already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for GPT-2 {phase}...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training for {phase} complete!\")\n",
    "\n",
    "    print(f\"Saving GPT-2 model for {phase}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"GPT-2 Model and tokenizer saved for {phase}.\")\n",
    "    return trainer\n",
    "\n",
    "# Head importance computation for GPT-2\n",
    "def compute_head_importance_gpt2(trainer: Trainer, eval_dataset) -> Dict[int, float]:\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        num_heads = model.config.n_head\n",
    "        head_importance[layer] = [0.0] * num_heads\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    # Normalize head importance\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    return head_importance\n",
    "\n",
    "# Pruning function for GPT-2\n",
    "def prune_automated_heads_gpt2(model, head_importance: Dict[int, float], threshold: float = 0.2, min_heads_to_retain: int = 2):\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < threshold]\n",
    "        \n",
    "        # Avoid pruning all heads from a layer\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - min_heads_to_retain:\n",
    "            print(f\"Warning: Not pruning layer {layer} to retain at least {min_heads_to_retain} heads.\")\n",
    "            heads_to_prune[layer] = []  # Skip this layer\n",
    "\n",
    "        print(f\"Layer {layer} Head Importance Scores: {importance_scores}\")\n",
    "\n",
    "    # Perform pruning\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Resize token embeddings if necessary\n",
    "    model.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained('gpt2')))\n",
    "    \n",
    "    # Reset model parameters if necessary\n",
    "    model.init_weights()  # Ensures weights are reinitialized after structure change\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluation function for GPT-2\n",
    "def evaluate_model_gpt2(model, validation_dataset):\n",
    "    model.eval()\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_eval_batch_size=4,\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args\n",
    "    )\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    return perplexity\n",
    "\n",
    "# Text generation function for GPT-2\n",
    "def generate_text_gpt2(model, tokenizer, input_text: str, max_length: int = 50, num_return_sequences: int = 3):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, \n",
    "                                 no_repeat_ngram_size=2, \n",
    "                                 early_stopping=True,\n",
    "                                 top_k=50, \n",
    "                                 top_p=0.95)\n",
    "\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Main execution for GPT-2\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the CNN/DailyMail dataset for GPT-2\n",
    "    train_data, validation_data = load_and_preprocess_data_gpt2(\"cnn_dailymail\", \"3.0.0\", 100, 50)\n",
    "\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_dataset, validation_dataset = tokenize_data_gpt2(train_data, validation_data, tokenizer)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    output_dir = \"./results-gpt2-automated-pruning\"\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Initial Training')\n",
    "\n",
    "    # Compute head importance\n",
    "    head_importance = compute_head_importance_gpt2(trainer, validation_dataset)\n",
    "\n",
    "    # Prune heads automatically based on importance\n",
    "    model = prune_automated_heads_gpt2(model, head_importance, threshold=0.05, min_heads_to_retain=1)\n",
    "\n",
    "    # Reinitialize the model after pruning\n",
    "    model.eval()\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer_pruned = train_and_save_model_gpt2(model, tokenizer, train_dataset, validation_dataset, output_dir, phase='Automated Pruning')\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    perplexity = evaluate_model_gpt2(model, validation_dataset)\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    input_text = \"The future of AI in healthcare is\"\n",
    "    generated_texts = generate_text_gpt2(model, tokenizer, input_text, max_length=200, num_return_sequences=1)\n",
    "\n",
    "    # Print the generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Model\n",
    "\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "from functools import partial\n",
    "\n",
    "# Set parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 4\n",
    "LR = 0.00005\n",
    "EPOCHS = 1\n",
    "MODEL = 'roberta-base'  # Change to 'roberta-base'\n",
    "OUT_DIR = 'arxiv_roberta'  # Update the output directory\n",
    "\n",
    "# Load datasets (using a sample for demonstration)\n",
    "train_dataset = load_dataset(\"ccdv/arxiv-classification\", split='train[:5%]')\n",
    "valid_dataset = load_dataset(\"ccdv/arxiv-classification\", split='validation[:10%]')\n",
    "test_dataset = load_dataset(\"ccdv/arxiv-classification\", split='test[:10%]')\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "            5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess_function(tokenizer, examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "tokenized_valid = valid_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "tokenized_test = test_dataset.map(partial(preprocess_function, tokenizer), batched=True, num_proc=NUM_PROCS)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Metric computation\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Model training setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=11, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to='tensorboard',\n",
    "    # Remove the following line\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Pruning function for dynamic pruning\n",
    "def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "    \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "    for step in range(steps):\n",
    "        print(f\"Pruning step {step + 1}/{steps}\")\n",
    "        # Prune weights in all linear layers\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        # Fine-tune the model after pruning\n",
    "        trainer.train()\n",
    "        # Remove pruning and make it permanent\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "# Apply dynamic pruning\n",
    "dynamic_prune_model(model, amount=0.1, steps=3)\n",
    "\n",
    "# Save the pruned model\n",
    "model.save_pretrained('pruned_arxiv_roberta')\n",
    "tokenizer.save_pretrained('pruned_arxiv_roberta')\n",
    "\n",
    "# Load the pruned model for inference\n",
    "model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_roberta')\n",
    "tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_roberta')\n",
    "classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Evaluate the pruned model on the test dataset\n",
    "eval_result = trainer.evaluate(tokenized_test)\n",
    "print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "# Inference with the pruned model\n",
    "all_files = glob.glob('inference_data/*')\n",
    "for file_name in all_files:\n",
    "    with open(file_name) as file:\n",
    "        content = file.read()\n",
    "        result = classify(content)\n",
    "        print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=1,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and prune the ALBERT model for text classification on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to use.\n",
    "        model_name (str): Pre-trained model name.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "        pruning_amount (float): Fraction of weights to prune in each step.\n",
    "        pruning_steps (int): Number of pruning steps.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split='train[:95%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split='validation[:100%]')\n",
    "    test_dataset = load_dataset(dataset_name, split='test[:100%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=11, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_albert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_albert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                content = file.read()\n",
    "                result = classify(content)\n",
    "                print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Call the function with custom arguments\n",
    "run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=50,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DistilBERT Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def run_distilbert_pipeline(dataset_name=\"ccdv/arxiv-classification\", \n",
    "                            train_split='train[:95%]', \n",
    "                            valid_split='validation[:100%]', \n",
    "                            test_split='test[:100%]', \n",
    "                            model_name='distilbert-base-uncased', \n",
    "                            batch_size=32, \n",
    "                            num_procs=4, \n",
    "                            lr=0.00005, \n",
    "                            epochs=1, \n",
    "                            prune_amount=0.1, \n",
    "                            prune_steps=3, \n",
    "                            out_dir='arxiv_distilbert'):\n",
    "    \"\"\"\n",
    "    Executes the DistilBERT model pipeline including dataset loading, tokenization,\n",
    "    model training, dynamic pruning, evaluation, and inference.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        test_split (str): Slice of the test data.\n",
    "        model_name (str): Name of the pretrained DistilBERT model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        prune_amount (float): Fraction of weights to prune.\n",
    "        prune_steps (int): Number of pruning steps.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    valid_dataset = load_dataset(dataset_name, split=valid_split)\n",
    "    test_dataset = load_dataset(dataset_name, split=test_split)\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Pruning function for dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=prune_amount, steps=prune_steps):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_distilbert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_distilbert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Call the function with specified arguments\n",
    "  run_distilbert_pipeline(\n",
    "      dataset_name=\"ccdv/arxiv-classification\", \n",
    "      train_split='train[:95%]', \n",
    "      valid_split='validation[:85%]', \n",
    "      test_split='test[:85%]', \n",
    "      model_name='distilbert-base-uncased', \n",
    "      batch_size=32, \n",
    "      num_procs=4, \n",
    "      lr=0.00005, \n",
    "      epochs=50, \n",
    "      prune_amount=0.1, \n",
    "      prune_steps=3, \n",
    "      out_dir='arxiv_distilbert'\n",
    ")\n",
    "\n",
    "# T5 Model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def run_t5_model_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str):\n",
    "    \"\"\"\n",
    "    Executes the T5 model pipeline including data loading, preprocessing, training, head importance computation, pruning, and evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., \"cnn_dailymail\").\n",
    "        version (str): Version of the dataset to load.\n",
    "        sample_train_size (int): Number of training samples to use.\n",
    "        sample_val_size (int): Number of validation samples to use.\n",
    "        output_dir (str): Directory to save the trained model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints results and saves the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"input_text\": x['article'], \"target_text\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"T5 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(examples['input_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        labels = tokenizer(examples['target_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "        return inputs\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Training\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"T5 Training already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for T5...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training complete!\")\n",
    "\n",
    "    print(f\"Saving T5 model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"T5 Model and tokenizer saved.\")\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {layer: [0.0] * model.config.num_heads for layer in range(model.config.num_layers)}\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, output_attentions=True, return_dict=True)\n",
    "            encoder_attentions = outputs.encoder_attentions\n",
    "            decoder_attentions = outputs.decoder_attentions\n",
    "\n",
    "            for layer_idx in range(len(encoder_attentions)):\n",
    "                layer_mean = encoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_mean[head_idx].item()\n",
    "\n",
    "            for layer_idx in range(len(decoder_attentions)):\n",
    "                layer_dec_mean = decoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_dec_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_dec_mean[head_idx].item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        if total > 0:\n",
    "            head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "        else:\n",
    "            head_importance[layer] = [0.0] * model.config.num_heads\n",
    "\n",
    "    heads_to_prune = {layer: [head_idx for head_idx, score in enumerate(head_importance[layer]) if score < 0.05] for layer in head_importance}\n",
    "    \n",
    "    # Prune heads\n",
    "    for layer, heads in heads_to_prune.items():\n",
    "        if heads:\n",
    "            model.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "            model.decoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "\n",
    "    model.eval()\n",
    "    trainer_pruned = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer_pruned.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"T5 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_t5_model_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-t5-automated-pruning\")\n",
    "\n",
    "\n",
    "\n",
    "# RoBERTa Model\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def roberta_pipeline(model_name: str, \n",
    "                    dataset_name: str, \n",
    "                    sample_fraction: float, \n",
    "                    out_dir: str, \n",
    "                    batch_size: int = 16, \n",
    "                    num_epochs: int = 5, \n",
    "                    learning_rate: float = 5e-5,\n",
    "                    pruning_amount: float = 0.1, \n",
    "                    pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete RoBERTa pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained RoBERTa model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "# Main execution for RoBERTa\n",
    "if __name__ == \"__main__\":\n",
    "    roberta_pipeline(\n",
    "        model_name='roberta-base',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.95,  # Use 95% of training data\n",
    "        out_dir='arxiv_roberta',\n",
    "        batch_size=16,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "\n",
    "\n",
    "# BERT Model\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def bert_pipeline(model_name: str, \n",
    "                 dataset_name: str, \n",
    "                 sample_fraction: float, \n",
    "                 out_dir: str, \n",
    "                 batch_size: int = 32, \n",
    "                 num_epochs: int = 3, \n",
    "                 learning_rate: float = 5e-5,\n",
    "                 pruning_amount: float = 0.1, \n",
    "                 pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete BERT pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained BERT model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "# Main execution for BERT\n",
    "if __name__ == \"__main__\":\n",
    "    bert_pipeline(\n",
    "        model_name='bert-base-uncased',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.8,\n",
    "        out_dir='arxiv_bert',\n",
    "        batch_size=32,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "\n",
    "\n",
    "# GPT2 Model\n",
    "import os\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def gpt2_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str, input_text: str):\n",
    "    \"\"\"Complete GPT-2 pipeline including loading data, training, pruning, evaluation, and text generation.\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    \n",
    "    # Load tokenizer and set padding token\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_data(data):\n",
    "        def tokenize_function(examples):\n",
    "            tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "            tokens['labels'] = tokens['input_ids'].copy()\n",
    "            return tokens\n",
    "\n",
    "        return data.map(tokenize_function, batched=True).set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    train_dataset = tokenize_data(train_data)\n",
    "    validation_dataset = tokenize_data(validation_data)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        head_importance[layer] = [0.0] * model.config.n_head\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    # Prune heads based on importance\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < 0.2]\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - 1:\n",
    "            heads_to_prune[layer] = []  # Skip pruning if too many heads would be removed\n",
    "\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Text generation\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Main execution for GPT-2\n",
    "if __name__ == \"__main__\":\n",
    "    generated = gpt2_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-gpt2-automated-pruning\", \"The future of AI in healthcare is\")\n",
    "    for i, text in enumerate(generated):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n",
    "\n",
    "\n",
    "\n",
    "# GPT2 model: Perplexity: 25.96\n",
    "# BERT Model: Accuracy: 85.95\n",
    "# RoBERTa Model: Accuracy: 81.69\n",
    "# T5Model: Perplexity: 155.25\n",
    "# DistilBERT Model: Accuracy: 89.25\n",
    "# ALBERT Model: Accuracy: 75.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 model: Perplexity: 25.96\n",
    "# BERT Model: Accuracy: 85.95\n",
    "# RoBERTa Model: Accuracy: 81.69\n",
    "# T5Model: Perplexity: 155.25\n",
    "# DistilBERT Model: Accuracy: 89.25\n",
    "# ALBERT Model: Accuracy: 75.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Results\n",
    "\n",
    "This repository contains the implementation and evaluation of various transformer models for natural language processing tasks. The primary models evaluated include GPT-2, BERT, RoBERTa, T5, DistilBERT, and ALBERT.\n",
    "\n",
    "## Models and Evaluation Metrics\n",
    "\n",
    "The following models have been evaluated with their respective metrics:\n",
    "\n",
    "| Model                | Evaluation Accuracy | Perplexity | Model Size After Pruning |\n",
    "|----------------------|---------------------|------------|--------------------------|\n",
    "| ALBERT               | 75.69%              | -          | 11 MB                    |\n",
    "| BERT                 | 85.95%              | -          | 420 MB                   |\n",
    "| DistilBERT           | 89.25%              | -          | 66 MB                    |\n",
    "| RoBERTa              | 81.69%              | -          | 500 MB                   |\n",
    "| GPT-2                | -                   | 25.96      | 345 MB                   |\n",
    "| T5                   | -                   | 155.25     | 220 MB                   |\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To run the models in this repository, you will need the following Python packages:\n",
    "\n",
    "- `torch`\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `evaluate`\n",
    "- `numpy`\n",
    "\n",
    "You can install the required packages using pip:\n",
    "\n",
    "```bash\n",
    "pip install torch transformers datasets evaluate numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using Unsloth for fine-tuning the LLM-transformer model\n",
    "# !pip install unsloth\n",
    "# # Also get the latest nightly Unsloth!\n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# def prune_and_fine_tune_llm_qwen2_5_or_llama3_or_phi3_5(model_name = \"unsloth/Qwen2.5-0.5B\",dataset_name = \"mlabonne/FineTome-100k\",epochs = 5):\n",
    "#     try:\n",
    "#         if \"Qwen2\" in model_name:\n",
    "#             chat_template = \"qwen-2.5\"\n",
    "#         elif \"Llama-3.2\" in model_name:\n",
    "#             chat_template = \"llama-3.1\"\n",
    "#         elif \"Llama\" in model_name:\n",
    "#             chat_template = \"llama-3.1\"\n",
    "#         elif \"Phi\" in model_name:\n",
    "#             chat_template = \"phi-3\"\n",
    "\n",
    "\n",
    "#         if \"Qwen2\" in model_name:\n",
    "#             instruction_part = \"<|im_start|>user\\n\"\n",
    "#             response_part = \"<|im_start|>assistant\\n\"\n",
    "#         elif \"Llama-3.2\" in model_name:\n",
    "#             instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "#             response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "#         elif \"Llama\" in model_name:\n",
    "#             instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "#             response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "#         elif \"Phi\" in model_name:\n",
    "#             instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "#             response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        \n",
    "\n",
    "#         from unsloth import FastLanguageModel\n",
    "#         import torch\n",
    "#         max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "#         dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "#         load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#         # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "#         fourbit_models = [\n",
    "#             \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "#             \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "#             \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "#             \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "#             \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "#             \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "#             \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "#             \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "#             \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "#             \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "#             \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "#             \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "#         ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "#         model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#             # Can select any from the below:\n",
    "#             # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "#             # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "#             # And also all Instruct versions and Math. Coding verisons!\n",
    "#             model_name = model_name,\n",
    "#             max_seq_length = max_seq_length,\n",
    "#             dtype = dtype,\n",
    "#             load_in_4bit = load_in_4bit,\n",
    "#             # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#         )\n",
    "\n",
    "#         model = FastLanguageModel.get_peft_model(\n",
    "#                     model,\n",
    "#                     r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#                     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                                     \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#                     lora_alpha = 16,\n",
    "#                     lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "#                     bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "#                     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#                     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#                     random_state = 3407,\n",
    "#                     use_rslora = False,  # We support rank stabilized LoRA\n",
    "#                     loftq_config = None, # And LoftQ\n",
    "#                 )\n",
    "\n",
    "\n",
    "#         def formatting_prompts_func(examples):\n",
    "#             convos = examples[\"conversations\"]\n",
    "#             texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "#             return { \"text\" : texts, }\n",
    "#         pass\n",
    "\n",
    "#         from datasets import load_dataset\n",
    "#         dataset = load_dataset(dataset_name, split = \"train\")\n",
    "#         from unsloth.chat_templates import standardize_sharegpt\n",
    "#         dataset = standardize_sharegpt(dataset)\n",
    "#         dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "#         from rich import print\n",
    "#         print(dataset)\n",
    "#         print(dataset[5][\"conversations\"])\n",
    "#         print(dataset[5][\"text\"])\n",
    "#         from trl import SFTTrainer\n",
    "#         from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "#         from unsloth import is_bfloat16_supported\n",
    "\n",
    "#         trainer = SFTTrainer(\n",
    "#             model = model,\n",
    "#             tokenizer = tokenizer,\n",
    "#             train_dataset = dataset,\n",
    "#             dataset_text_field = \"text\",\n",
    "#             max_seq_length = max_seq_length,\n",
    "#             data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "#             dataset_num_proc = 2,\n",
    "#             packing = False, # Can make training 5x faster for short sequences.\n",
    "#             args = TrainingArguments(\n",
    "#                 per_device_train_batch_size = 2,\n",
    "#                 gradient_accumulation_steps = 4,\n",
    "#                 warmup_steps = epochs,\n",
    "#                 num_train_epochs = 10, # Set this for 1 full training run.\n",
    "#                 # max_steps = 60,\n",
    "#                 learning_rate = 2e-4,\n",
    "#                 fp16 = not is_bfloat16_supported(),\n",
    "#                 bf16 = is_bfloat16_supported(),\n",
    "#                 logging_steps = 1,\n",
    "#                 optim = \"adamw_8bit\",\n",
    "#                 weight_decay = 0.01,\n",
    "#                 lr_scheduler_type = \"linear\",\n",
    "#                 seed = 3407,\n",
    "#                 output_dir = \"outputs\",\n",
    "#             ),\n",
    "#         )\n",
    "#         from unsloth.chat_templates import train_on_responses_only\n",
    "#         trainer = train_on_responses_only(\n",
    "#             trainer,\n",
    "#             instruction_part = instruction_part,\n",
    "#             response_part = response_part,\n",
    "#         )\n",
    "#         print(tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]))\n",
    "#         space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "#         print(tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]))\n",
    "#         #@title Show current memory stats\n",
    "#         gpu_stats = torch.cuda.get_device_properties(0)\n",
    "#         start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "#         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "#         print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "#         print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "#         trainer_stats = trainer.train()\n",
    "#         #@title Show final memory and time stats\n",
    "#         used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "#         used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "#         used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "#         lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "#         print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "#         print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "#         print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "#         print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "#         print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "#         print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "#         from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "#         tokenizer = get_chat_template(\n",
    "#             tokenizer,\n",
    "#             chat_template = chat_template,\n",
    "#         )\n",
    "#         FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "#         ]\n",
    "#         inputs = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize = True,\n",
    "#             add_generation_prompt = True, # Must add for generation\n",
    "#             return_tensors = \"pt\",\n",
    "#         ).to(\"cuda\")\n",
    "\n",
    "#         outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "#                                 temperature = 1.5, min_p = 0.1)\n",
    "#         print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "#         FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "#         ]\n",
    "#         inputs = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize = True,\n",
    "#             add_generation_prompt = True, # Must add for generation\n",
    "#             return_tensors = \"pt\",\n",
    "#         ).to(\"cuda\")\n",
    "\n",
    "#         from transformers import TextStreamer\n",
    "#         text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "#         _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "#                         use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "        \n",
    "#         model.save_pretrained(\"lora_model\") # Local saving\n",
    "#         tokenizer.save_pretrained(\"lora_model\")\n",
    "#         # :Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:\n",
    "#         if True:\n",
    "#             from unsloth import FastLanguageModel\n",
    "#             model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#                 model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "#                 max_seq_length = max_seq_length,\n",
    "#                 dtype = dtype,\n",
    "#                 load_in_4bit = load_in_4bit,\n",
    "#             )\n",
    "#             FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": \"write a basic python program with CURD operations.\"},\n",
    "#         ]\n",
    "#         inputs = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize = True,\n",
    "#             add_generation_prompt = True, # Must add for generation\n",
    "#             return_tensors = \"pt\",\n",
    "#         ).to(\"cuda\")\n",
    "\n",
    "#         from transformers import TextStreamer\n",
    "#         text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "#         _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 500,\n",
    "#                         use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "# prune_and_fine_tune_llm_qwen2_5_or_llama3_or_phi3_5(model_name = \"unsloth/Qwen2.5-0.5B\",dataset_name = \"mlabonne/FineTome-100k\",epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "def train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", epochs=5, output_dir=\"lora_model\", max_steps=150):\n",
    "    \"\"\"Train a language model using fine-tuning techniques.\"\"\"\n",
    "    try:\n",
    "        # Set chat_template and related prompt formatting\n",
    "        if \"Qwen2\" in model_name:\n",
    "            chat_template = \"qwen-2.5\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            chat_template = \"llama-3.1\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            chat_template = \"phi-3\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        # Define instruction and response parts for different model types\n",
    "        if \"Qwen2\" in model_name:\n",
    "            instruction_part = \"<|im_start|>user\\n\"\n",
    "            response_part = \"<|im_start|>assistant\\n\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            dtype = None\n",
    "        )\n",
    "\n",
    "        # Set the chat template for the tokenizer\n",
    "        tokenizer.chat_template = chat_template\n",
    "\n",
    "        # Configure PEFT\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            use_rslora = False,  # We support rank stabilized LoRA\n",
    "            loftq_config = None, # And LoftQ\n",
    "        )\n",
    "        from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template = chat_template,\n",
    "        )\n",
    "        # Dataset formatting\n",
    "        def formatting_prompts_func(examples):\n",
    "            convos = examples[\"conversations\"]\n",
    "            texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "            return {\"text\": texts}\n",
    "\n",
    "        dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        dataset = standardize_sharegpt(dataset)\n",
    "        dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        print(dataset)\n",
    "        print(dataset[5][\"conversations\"])\n",
    "        print(dataset[5][\"text\"])\n",
    "        # Configure the trainer\n",
    "        trainer = SFTTrainer(\n",
    "                        model = model,\n",
    "                        tokenizer = tokenizer,\n",
    "                        train_dataset = dataset,\n",
    "                        dataset_text_field = \"text\",\n",
    "                        max_seq_length = 2048,\n",
    "                        data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "                        dataset_num_proc = 2,\n",
    "                        packing = False, # Can make training 5x faster for short sequences.\n",
    "                        args = TrainingArguments(\n",
    "                            per_device_train_batch_size = 2,\n",
    "                            gradient_accumulation_steps = 4,\n",
    "                            warmup_steps = 5,\n",
    "                            num_train_epochs = epochs, # Set this for 1 full training run.\n",
    "                            # max_steps = max_steps,\n",
    "                            learning_rate = 2e-4,\n",
    "                            fp16 = not is_bfloat16_supported(),\n",
    "                            bf16 = is_bfloat16_supported(),\n",
    "                            logging_steps = 1,\n",
    "                            optim = \"adamw_8bit\",\n",
    "                            weight_decay = 0.01,\n",
    "                            lr_scheduler_type = \"linear\",\n",
    "                            seed = 3407,\n",
    "                            output_dir = \"outputs\",\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "        # Train on responses only\n",
    "        trainer = train_on_responses_only(trainer, instruction_part=instruction_part, response_part=response_part)\n",
    "\n",
    "        print(tokenizer.decode(trainer.train_dataset[5][\"input_ids\"]))\n",
    "        space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "        print(tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]))\n",
    "\n",
    "        # Show GPU memory usage\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "        # Train the model\n",
    "        trainer_stats = trainer.train()\n",
    "\n",
    "        # Show memory usage after training\n",
    "        #@title Show final memory and time stats\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}.\")\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "model_output_dir = train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", max_steps=150, epochs=5, output_dir=\"lora_model\")\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "def inference_llm(model_dir=\"lora_model\", input_text=\"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"):\n",
    "    \"\"\"Perform inference using the fine-tuned language model.\"\"\"\n",
    "    try:\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_dir,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "        # Set up the chat template\n",
    "        chat_template = get_chat_template(tokenizer)\n",
    "\n",
    "        # Prepare the input message\n",
    "        messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Generate output\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=500, use_cache=True, temperature=1.5, min_p=0.1)\n",
    "        print(\"Inference completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "\n",
    "# Example usage\n",
    "inference_llm(model_dir=model_output_dir, input_text=\"write a fastapi program with all the routes such as post, get, put, delete make use of in memory data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "def train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", epochs=5, output_dir=\"lora_model\", max_steps=150):\n",
    "    \"\"\"\n",
    "    Finetune Qwne2.5, Llama 3.2, Mistral, Phi-3.5 & Gemma 2-5x faster with 80% less memory!\n",
    "    Train a language model using fine-tuning techniques with parameter-efficient fine-tuning (PEFT).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained language model to fine-tune. \n",
    "                          It should be compatible with the `FastLanguageModel` library.\n",
    "                          Default is \"unsloth/Qwen2.5-0.5B\".\n",
    "        dataset_name (str): The name of the dataset to use for fine-tuning. \n",
    "                            It should be compatible with the `datasets` library. \n",
    "                            Default is \"mlabonne/FineTome-100k\".\n",
    "        epochs (int): The number of training epochs to perform. \n",
    "                      Currently unused; `max_steps` is used to control the training length.\n",
    "                      Default is 5.\n",
    "        output_dir (str): The directory where the fine-tuned model and tokenizer will be saved.\n",
    "                          Default is \"lora_model\".\n",
    "        max_steps (int): The maximum number of training steps. \n",
    "                         Controls the length of the training process.\n",
    "                         Default is 150.\n",
    "\n",
    "    Returns:\n",
    "        str: The directory where the fine-tuned model is saved if training is successful.\n",
    "        None: If an error occurs during the training process.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified `model_name` is not supported.\n",
    "\n",
    "    Description:\n",
    "        The function fine-tunes a pre-trained language model using PEFT techniques such as \n",
    "        Low-Rank Adaptation (LoRA). Depending on the model type, it sets up a suitable chat \n",
    "        template, instruction format, and response format. It uses a dataset of conversations \n",
    "        or prompts, formats the data according to the model's expected input structure, and \n",
    "        configures the training using the `SFTTrainer` from the `trl` library. The function \n",
    "        handles different dataset formatting for specific models like \"mistral\" using \n",
    "        Alpaca-like prompt templates.\n",
    "\n",
    "    Notes:\n",
    "        - If a GPU is available, memory usage statistics are printed before and after training.\n",
    "        - The trained model and tokenizer are saved to the specified `output_dir`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set chat_template and related prompt formatting\n",
    "        if \"Qwen2\" in model_name:\n",
    "            chat_template = \"qwen-2.5\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            chat_template = \"llama-3.1\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            chat_template = \"phi-3\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        # Define instruction and response parts for different model types\n",
    "        if \"Qwen2\" in model_name:\n",
    "            instruction_part = \"<|im_start|>user\\n\"\n",
    "            response_part = \"<|im_start|>assistant\\n\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "        )\n",
    "\n",
    "        # Set the chat template for the tokenizer\n",
    "        tokenizer.chat_template = chat_template\n",
    "\n",
    "        # Configure PEFT\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None  # And LoftQ\n",
    "        )\n",
    "        \n",
    "        # Dataset formatting\n",
    "        if \"mistral\" not in model_name:\n",
    "            from unsloth.chat_templates import get_chat_template\n",
    "            tokenizer = get_chat_template(\n",
    "              tokenizer,\n",
    "              chat_template = chat_template,)\n",
    "            def formatting_prompts_func(examples):\n",
    "                convos = examples[\"conversations\"]\n",
    "                texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "                return {\"text\": texts}\n",
    "\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "            dataset = standardize_sharegpt(dataset)\n",
    "            dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        else:\n",
    "            alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "            ### Instruction:\n",
    "            {}\n",
    "\n",
    "            ### Input:\n",
    "            {}\n",
    "\n",
    "            ### Response:\n",
    "            {}\"\"\"\n",
    "            \n",
    "            EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "            \n",
    "            def formatting_prompts_func(examples):\n",
    "                instructions = examples[\"instruction\"]\n",
    "                inputs = examples[\"input\"]\n",
    "                outputs = examples[\"output\"]\n",
    "                texts = []\n",
    "                for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "                    text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "                    texts.append(text)\n",
    "                return {\"text\": texts}\n",
    "\n",
    "            dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "            dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "        # Configure the trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "            dataset_num_proc=2,\n",
    "            packing=False,  # Can make training 5x faster for short sequences.\n",
    "            args=TrainingArguments(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=5,\n",
    "                num_train_epochs = epochs, # Set this for 1 full training run.\n",
    "                # max_steps = max_steps, # Set num_train_epochs = 1 for full training runs\n",
    "                learning_rate=2e-4,\n",
    "                fp16=not is_bfloat16_supported(),\n",
    "                bf16=is_bfloat16_supported(),\n",
    "                logging_steps=1,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type=\"linear\",\n",
    "                seed=3407,\n",
    "                output_dir=output_dir,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Train on responses only\n",
    "        trainer = train_on_responses_only(trainer, instruction_part=instruction_part, response_part=response_part)\n",
    "\n",
    "        # GPU memory usage before training\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "        # Train the model\n",
    "        trainer_stats = trainer.train()\n",
    "\n",
    "        # GPU memory usage after training\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}.\")\n",
    "\n",
    "        return output_dir\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "model_output_dir = train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", max_steps=1000, epochs=5, output_dir=\"lora_models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "def inference_llm(model_dir=\"lora_models\", input_text=\"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\", max_new_tokens = 500):\n",
    "    \"\"\"\n",
    "    Generate text using a fine-tuned language model for inference.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): The directory where the fine-tuned model and tokenizer are stored.\n",
    "                         Default is \"lora_models\".\n",
    "        input_text (str): The prompt or input text to feed into the language model for generating a response.\n",
    "                          Default is \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\".\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate in the output.\n",
    "                              Default is 500.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text output from the model based on the provided input prompt.\n",
    "\n",
    "    Description:\n",
    "        This function loads a fine-tuned language model and its corresponding tokenizer \n",
    "        from the specified directory (`model_dir`). It uses the model to generate a text \n",
    "        continuation based on the given `input_text`, limiting the generation to \n",
    "        `max_new_tokens`. The function is useful for performing text generation tasks, \n",
    "        such as dialogue completion, text summarization, or sequence generation.\n",
    "\n",
    "    Notes:\n",
    "        - Ensure that the fine-tuned model and tokenizer are saved in the specified `model_dir`.\n",
    "        - The generation configuration may include additional parameters like temperature, \n",
    "          top-k sampling, or top-p sampling, which can be adjusted within the function \n",
    "          to control the diversity of the output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_dir,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "        # Set up the chat template\n",
    "        chat_template = get_chat_template(tokenizer)\n",
    "\n",
    "        # Prepare the input message\n",
    "        messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Generate output\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=max_new_tokens, use_cache=True, temperature=1.5, min_p=0.1)\n",
    "        print(\"Inference completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "\n",
    "# Example usage\n",
    "inference_llm(model_dir=model_output_dir, input_text=\"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\", max_new_tokens = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from typing import Dict, List, Tuple\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "\n",
    "from rich import print\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only, get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "\n",
    "\n",
    "def train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", epochs=5, output_dir=\"lora_model\", max_steps=150):\n",
    "    \"\"\"\n",
    "    Finetune Qwne2.5, Llama 3.2, Mistral, Phi-3.5 & Gemma 2-5x faster with 80% less memory!\n",
    "    Train a language model using fine-tuning techniques with parameter-efficient fine-tuning (PEFT).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained language model to fine-tune.\n",
    "                          It should be compatible with the `FastLanguageModel` library.\n",
    "                          Default is \"unsloth/Qwen2.5-0.5B\".\n",
    "        dataset_name (str): The name of the dataset to use for fine-tuning.\n",
    "                            It should be compatible with the `datasets` library.\n",
    "                            Default is \"mlabonne/FineTome-100k\".\n",
    "        epochs (int): The number of training epochs to perform.\n",
    "                      Currently unused; `max_steps` is used to control the training length.\n",
    "                      Default is 5.\n",
    "        output_dir (str): The directory where the fine-tuned model and tokenizer will be saved.\n",
    "                          Default is \"lora_model\".\n",
    "        max_steps (int): The maximum number of training steps.\n",
    "                         Controls the length of the training process.\n",
    "                         Default is 150.\n",
    "\n",
    "    Returns:\n",
    "        str: The directory where the fine-tuned model is saved if training is successful.\n",
    "        None: If an error occurs during the training process.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified `model_name` is not supported.\n",
    "\n",
    "    Description:\n",
    "        The function fine-tunes a pre-trained language model using PEFT techniques such as\n",
    "        Low-Rank Adaptation (LoRA). Depending on the model type, it sets up a suitable chat\n",
    "        template, instruction format, and response format. It uses a dataset of conversations\n",
    "        or prompts, formats the data according to the model's expected input structure, and\n",
    "        configures the training using the `SFTTrainer` from the `trl` library. The function\n",
    "        handles different dataset formatting for specific models like \"mistral\" using\n",
    "        Alpaca-like prompt templates.\n",
    "\n",
    "    Notes:\n",
    "        - If a GPU is available, memory usage statistics are printed before and after training.\n",
    "        - The trained model and tokenizer are saved to the specified `output_dir`.\n",
    "    \"\"\"\n",
    "    # unsloth/Llama-3.2-1B-Instruct\n",
    "    # unsloth/Llama-3.2-3B-Instruct\n",
    "    try:\n",
    "        # Set chat_template and related prompt formatting\n",
    "        if \"Qwen2\" in model_name:\n",
    "            chat_template = \"qwen-2.5\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            chat_template = \"llama-3.1\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            chat_template = \"phi-3\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        # Define instruction and response parts for different model types\n",
    "        if \"Qwen2\" in model_name:\n",
    "            instruction_part = \"<|im_start|>user\\n\"\n",
    "            response_part = \"<|im_start|>assistant\\n\"\n",
    "        elif \"Llama\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        elif \"Phi\" in model_name:\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "        )\n",
    "\n",
    "        # Set the chat template for the tokenizer\n",
    "        tokenizer.chat_template = chat_template\n",
    "\n",
    "        # Configure PEFT\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None  # And LoftQ\n",
    "        )\n",
    "\n",
    "        # Dataset formatting\n",
    "        if \"mistral\" not in model_name:\n",
    "            from unsloth.chat_templates import get_chat_template\n",
    "            tokenizer = get_chat_template(\n",
    "              tokenizer,\n",
    "              chat_template = chat_template,)\n",
    "            def formatting_prompts_func(examples):\n",
    "                convos = examples[\"conversations\"]\n",
    "                texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "                return {\"text\": texts}\n",
    "\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "            dataset = standardize_sharegpt(dataset)\n",
    "            dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        else:\n",
    "            alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "            ### Instruction:\n",
    "            {}\n",
    "\n",
    "            ### Input:\n",
    "            {}\n",
    "\n",
    "            ### Response:\n",
    "            {}\"\"\"\n",
    "\n",
    "            EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "            def formatting_prompts_func(examples):\n",
    "                instructions = examples[\"instruction\"]\n",
    "                inputs = examples[\"input\"]\n",
    "                outputs = examples[\"output\"]\n",
    "                texts = []\n",
    "                for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "                    text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "                    texts.append(text)\n",
    "                return {\"text\": texts}\n",
    "\n",
    "            dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "            dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "        # Configure the trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "            dataset_num_proc=2,\n",
    "            packing=False,  # Can make training 5x faster for short sequences.\n",
    "            args=TrainingArguments(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=5,\n",
    "                num_train_epochs = epochs, # Set this for 1 full training run.\n",
    "                # max_steps = max_steps, # Set num_train_epochs = 1 for full training runs\n",
    "                learning_rate=2e-4,\n",
    "                fp16=not is_bfloat16_supported(),\n",
    "                bf16=is_bfloat16_supported(),\n",
    "                logging_steps=1,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type=\"linear\",\n",
    "                seed=3407,\n",
    "                output_dir=output_dir,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Train on responses only\n",
    "        trainer = train_on_responses_only(trainer, instruction_part=instruction_part, response_part=response_part)\n",
    "\n",
    "        # GPU memory usage before training\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "        # Train the model\n",
    "        trainer_stats = trainer.train()\n",
    "\n",
    "        # GPU memory usage after training\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}.\")\n",
    "\n",
    "        return output_dir\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None\n",
    "    \n",
    "def inference_llm(model_dir=\"lora_models\", input_text=\"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\", max_new_tokens = 500):\n",
    "    \"\"\"\n",
    "    Generate text using a fine-tuned language model for inference.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): The directory where the fine-tuned model and tokenizer are stored.\n",
    "                         Default is \"lora_models\".\n",
    "        input_text (str): The prompt or input text to feed into the language model for generating a response.\n",
    "                          Default is \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\".\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate in the output.\n",
    "                              Default is 500.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text output from the model based on the provided input prompt.\n",
    "\n",
    "    Description:\n",
    "        This function loads a fine-tuned language model and its corresponding tokenizer\n",
    "        from the specified directory (`model_dir`). It uses the model to generate a text\n",
    "        continuation based on the given `input_text`, limiting the generation to\n",
    "        `max_new_tokens`. The function is useful for performing text generation tasks,\n",
    "        such as dialogue completion, text summarization, or sequence generation.\n",
    "\n",
    "    Notes:\n",
    "        - Ensure that the fine-tuned model and tokenizer are saved in the specified `model_dir`.\n",
    "        - The generation configuration may include additional parameters like temperature,\n",
    "          top-k sampling, or top-p sampling, which can be adjusted within the function\n",
    "          to control the diversity of the output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_dir,\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "        # Set up the chat template\n",
    "        chat_template = get_chat_template(tokenizer)\n",
    "\n",
    "        # Prepare the input message\n",
    "        messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Generate output\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=max_new_tokens, use_cache=True, temperature=1.5, min_p=0.1)\n",
    "        print(\"Inference completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "\n",
    "\n",
    "def run_albert_pruning_pipeline(\n",
    "    dataset_name=\"ccdv/arxiv-classification\",\n",
    "    model_name='albert-base-v2',\n",
    "    out_dir='arxiv_albert',\n",
    "    batch_size=32,\n",
    "    num_procs=4,\n",
    "    lr=0.00005,\n",
    "    epochs=1,\n",
    "    pruning_amount=0.1,\n",
    "    pruning_steps=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and prune the ALBERT model for text classification on the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to use.\n",
    "        model_name (str): Pre-trained model name.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "        pruning_amount (float): Fraction of weights to prune in each step.\n",
    "        pruning_steps (int): Number of pruning steps.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split='train[:95%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split='validation[:100%]')\n",
    "    test_dataset = load_dataset(dataset_name, split='test[:100%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=11, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_albert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_albert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_albert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                content = file.read()\n",
    "                result = classify(content)\n",
    "                print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "def run_distilbert_pipeline(dataset_name=\"ccdv/arxiv-classification\", \n",
    "                            train_split='train[:95%]', \n",
    "                            valid_split='validation[:100%]', \n",
    "                            test_split='test[:100%]', \n",
    "                            model_name='distilbert-base-uncased', \n",
    "                            batch_size=32, \n",
    "                            num_procs=4, \n",
    "                            lr=0.00005, \n",
    "                            epochs=1, \n",
    "                            prune_amount=0.1, \n",
    "                            prune_steps=3, \n",
    "                            out_dir='arxiv_distilbert'):\n",
    "    \"\"\"\n",
    "    Executes the DistilBERT model pipeline including dataset loading, tokenization,\n",
    "    model training, dynamic pruning, evaluation, and inference.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_split (str): Slice of the training data.\n",
    "        valid_split (str): Slice of the validation data.\n",
    "        test_split (str): Slice of the test data.\n",
    "        model_name (str): Name of the pretrained DistilBERT model.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        num_procs (int): Number of processes for tokenization.\n",
    "        lr (float): Learning rate for training.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        prune_amount (float): Fraction of weights to prune.\n",
    "        prune_steps (int): Number of pruning steps.\n",
    "        out_dir (str): Output directory for saving the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints evaluation results and predictions for inference data.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=train_split)\n",
    "    valid_dataset = load_dataset(dataset_name, split=valid_split)\n",
    "    test_dataset = load_dataset(dataset_name, split=test_split)\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", \n",
    "                4: \"math.GR\", 5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", \n",
    "                8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True, num_proc=num_procs)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Pruning function for dynamic pruning\n",
    "    def dynamic_prune_model(model, amount=prune_amount, steps=prune_steps):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step + 1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer.save_pretrained('pruned_arxiv_distilbert')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('pruned_arxiv_distilbert')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pruned_arxiv_distilbert')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "\n",
    "def run_t5_model_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str):\n",
    "    \"\"\"\n",
    "    Executes the T5 model pipeline including data loading, preprocessing, training, head importance computation, pruning, and evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load (e.g., \"cnn_dailymail\").\n",
    "        version (str): Version of the dataset to load.\n",
    "        sample_train_size (int): Number of training samples to use.\n",
    "        sample_val_size (int): Number of validation samples to use.\n",
    "        output_dir (str): Directory to save the trained model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints results and saves the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"input_text\": x['article'], \"target_text\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    print(f\"T5 Training size: {len(train_data)}, Validation size: {len(validation_data)}\")\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(examples['input_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        labels = tokenizer(examples['target_text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "        return inputs\n",
    "\n",
    "    train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "    validation_dataset = validation_data.map(tokenize_function, batched=True)\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Training\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_dir) and any('checkpoint' in file for file in os.listdir(output_dir)):\n",
    "        print(f\"T5 Training already completed. Skipping...\")\n",
    "    else:\n",
    "        print(f\"Starting training for T5...\")\n",
    "        trainer.train()\n",
    "        print(f\"Training complete!\")\n",
    "\n",
    "    print(f\"Saving T5 model...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"T5 Model and tokenizer saved.\")\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {layer: [0.0] * model.config.num_heads for layer in range(model.config.num_layers)}\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, output_attentions=True, return_dict=True)\n",
    "            encoder_attentions = outputs.encoder_attentions\n",
    "            decoder_attentions = outputs.decoder_attentions\n",
    "\n",
    "            for layer_idx in range(len(encoder_attentions)):\n",
    "                layer_mean = encoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_mean[head_idx].item()\n",
    "\n",
    "            for layer_idx in range(len(decoder_attentions)):\n",
    "                layer_dec_mean = decoder_attentions[layer_idx].mean(dim=(0, 1, 2)).detach()\n",
    "                for head_idx in range(min(layer_dec_mean.size(0), len(head_importance[layer_idx]))):\n",
    "                    head_importance[layer_idx][head_idx] += layer_dec_mean[head_idx].item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        if total > 0:\n",
    "            head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "        else:\n",
    "            head_importance[layer] = [0.0] * model.config.num_heads\n",
    "\n",
    "    heads_to_prune = {layer: [head_idx for head_idx, score in enumerate(head_importance[layer]) if score < 0.05] for layer in head_importance}\n",
    "    \n",
    "    # Prune heads\n",
    "    for layer, heads in heads_to_prune.items():\n",
    "        if heads:\n",
    "            model.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "            model.decoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n",
    "\n",
    "    model.eval()\n",
    "    trainer_pruned = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer_pruned.evaluate(eval_dataset=validation_dataset)\n",
    "    print(f\"Eval Results: {eval_results}\")\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"T5 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "\n",
    "def roberta_pipeline(model_name: str, \n",
    "                    dataset_name: str, \n",
    "                    sample_fraction: float, \n",
    "                    out_dir: str, \n",
    "                    batch_size: int = 16, \n",
    "                    num_epochs: int = 5, \n",
    "                    learning_rate: float = 5e-5,\n",
    "                    pruning_amount: float = 0.1, \n",
    "                    pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete RoBERTa pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained RoBERTa model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 16.\n",
    "        num_epochs (int): Number of training epochs. Default is 5.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "def bert_pipeline(model_name: str, \n",
    "                 dataset_name: str, \n",
    "                 sample_fraction: float, \n",
    "                 out_dir: str, \n",
    "                 batch_size: int = 32, \n",
    "                 num_epochs: int = 3, \n",
    "                 learning_rate: float = 5e-5,\n",
    "                 pruning_amount: float = 0.1, \n",
    "                 pruning_steps: int = 3):\n",
    "    \"\"\"\n",
    "    Complete BERT pipeline including loading data, training, pruning, evaluation, and text classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained BERT model.\n",
    "        dataset_name (str): Name of the dataset to be loaded.\n",
    "        sample_fraction (float): Fraction of the dataset to use for training/validation/testing.\n",
    "        out_dir (str): Directory to save the model.\n",
    "        batch_size (int): Training batch size. Default is 32.\n",
    "        num_epochs (int): Number of training epochs. Default is 3.\n",
    "        learning_rate (float): Learning rate for training. Default is 5e-5.\n",
    "        pruning_amount (float): Fraction of weights to prune. Default is 0.1.\n",
    "        pruning_steps (int): Number of pruning steps to perform. Default is 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = load_dataset(dataset_name, split=f'train[:{int(sample_fraction * 100)}%]')\n",
    "    valid_dataset = load_dataset(dataset_name, split=f'validation[:{int(sample_fraction * 100)}%]')\n",
    "    test_dataset = load_dataset(dataset_name, split=f'test[:{int(sample_fraction * 100)}%]')\n",
    "\n",
    "    # Label mapping\n",
    "    id2label = {0: \"math.AC\", 1: \"cs.CV\", 2: \"cs.AI\", 3: \"cs.SY\", 4: \"math.GR\",\n",
    "                5: \"cs.CE\", 6: \"cs.PL\", 7: \"cs.IT\", 8: \"cs.DS\", 9: \"cs.NE\", 10: \"math.ST\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Metric computation\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Model training setup\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        report_to='tensorboard',\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Dynamic pruning function\n",
    "    def dynamic_prune_model(model, amount=0.2, steps=5):\n",
    "        \"\"\"Prune the model iteratively during training.\"\"\"\n",
    "        for step in range(steps):\n",
    "            print(f\"Pruning step {step+1}/{steps}\")\n",
    "            # Prune weights in all linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # Fine-tune the model after pruning\n",
    "            trainer.train()\n",
    "            # Remove pruning and make it permanent\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.remove(module, 'weight')\n",
    "\n",
    "    # Apply dynamic pruning\n",
    "    dynamic_prune_model(model, amount=pruning_amount, steps=pruning_steps)\n",
    "\n",
    "    # Save the pruned model\n",
    "    model.save_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer.save_pretrained(f'pruned_{out_dir}')\n",
    "\n",
    "    # Load the pruned model for inference\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f'pruned_{out_dir}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'pruned_{out_dir}')\n",
    "    classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate the pruned model on the test dataset\n",
    "    eval_result = trainer.evaluate(tokenized_test)\n",
    "    print(f\"Evaluation results after pruning: {eval_result}\")\n",
    "\n",
    "    # Inference with the pruned model\n",
    "    all_files = glob.glob('inference_data/*')\n",
    "    for file_name in all_files:\n",
    "        with open(file_name) as file:\n",
    "            content = file.read()\n",
    "            result = classify(content)\n",
    "            print(f'File: {file_name}, Prediction: {result}, Ground Truth: {file_name.split(\"_\")[-1].split(\".txt\")[0]}')\n",
    "\n",
    "\n",
    "def gpt2_pipeline(dataset_name: str, version: str, sample_train_size: int, sample_val_size: int, output_dir: str, input_text: str):\n",
    "    \"\"\"Complete GPT-2 pipeline including loading data, training, pruning, evaluation, and text generation.\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    dataset = load_dataset(dataset_name, version)\n",
    "    processed_data = dataset['train'].map(lambda x: {\"text\": x['article'], \"target\": x['highlights']})\n",
    "    split_data = processed_data.train_test_split(test_size=0.1)\n",
    "    train_data = split_data['train'].select(range(min(sample_train_size, len(split_data['train']))))\n",
    "    validation_data = split_data['test'].select(range(min(sample_val_size, len(split_data['test']))))\n",
    "    \n",
    "    # Load tokenizer and set padding token\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_data(data):\n",
    "        def tokenize_function(examples):\n",
    "            tokens = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "            tokens['labels'] = tokens['input_ids'].copy()\n",
    "            return tokens\n",
    "\n",
    "        return data.map(tokenize_function, batched=True).set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    train_dataset = tokenize_data(train_data)\n",
    "    validation_dataset = tokenize_data(validation_data)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Compute head importance\n",
    "    model.eval()\n",
    "    head_importance = {}\n",
    "    for layer in range(model.config.n_layer):\n",
    "        head_importance[layer] = [0.0] * model.config.n_head\n",
    "\n",
    "    dataloader = trainer.get_eval_dataloader(validation_dataset)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        for layer_idx, layer_att in enumerate(attentions):\n",
    "            layer_mean = layer_att.mean(dim=0)\n",
    "            for head_idx in range(layer_mean.size(0)):\n",
    "                head_importance[layer_idx][head_idx] += layer_mean[head_idx].mean().item()\n",
    "\n",
    "    for layer in head_importance:\n",
    "        total = sum(head_importance[layer])\n",
    "        head_importance[layer] = [head / total for head in head_importance[layer]]\n",
    "\n",
    "    # Prune heads based on importance\n",
    "    heads_to_prune = {}\n",
    "    for layer, importance_scores in head_importance.items():\n",
    "        heads_to_prune[layer] = [head_idx for head_idx, score in enumerate(importance_scores) if score < 0.2]\n",
    "        if len(heads_to_prune[layer]) >= model.config.n_head - 1:\n",
    "            heads_to_prune[layer] = []  # Skip pruning if too many heads would be removed\n",
    "\n",
    "    model.prune_heads(heads_to_prune)\n",
    "\n",
    "    # Train the pruned model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    eval_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
    "    perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()\n",
    "    print(f\"GPT-2 Automated Pruning Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Text generation\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Main execution for GPT-2\n",
    "if __name__ == \"__main__\":\n",
    "    # GPT2\n",
    "    generated = gpt2_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-gpt2-automated-pruning\", \"The future of AI in healthcare is\")\n",
    "    for i, text in enumerate(generated):\n",
    "        print(f\"Generated Text {i + 1}: {text}\")\n",
    "\n",
    "    # ALBERT Call the function with custom arguments\n",
    "    run_albert_pruning_pipeline(\n",
    "        dataset_name=\"ccdv/arxiv-classification\",\n",
    "        model_name='albert-base-v2',\n",
    "        out_dir='arxiv_albert',\n",
    "        batch_size=32,\n",
    "        num_procs=4,\n",
    "        lr=0.00005,\n",
    "        epochs=50,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "\n",
    "    # Main execution for BERT\n",
    "    bert_pipeline(\n",
    "        model_name='bert-base-uncased',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.8,\n",
    "        out_dir='arxiv_bert',\n",
    "        batch_size=32,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "    # Main execution for RoBERTa\n",
    "    roberta_pipeline(\n",
    "        model_name='roberta-base',\n",
    "        dataset_name='ccdv/arxiv-classification',\n",
    "        sample_fraction=0.95,  # Use 95% of training data\n",
    "        out_dir='arxiv_roberta',\n",
    "        batch_size=16,\n",
    "        num_epochs=50,\n",
    "        learning_rate=5e-5,\n",
    "        pruning_amount=0.1,\n",
    "        pruning_steps=3\n",
    "    )\n",
    "    # Main execution for T5 Model\n",
    "    run_t5_model_pipeline(\"cnn_dailymail\", \"3.0.0\", 27000, 10000, \"./results-t5-automated-pruning\")\n",
    "\n",
    "    \n",
    "    # Call the function with specified arguments\n",
    "    run_distilbert_pipeline(\n",
    "      dataset_name=\"ccdv/arxiv-classification\", \n",
    "      train_split='train[:95%]', \n",
    "      valid_split='validation[:85%]', \n",
    "      test_split='test[:85%]', \n",
    "      model_name='distilbert-base-uncased', \n",
    "      batch_size=32, \n",
    "      num_procs=4, \n",
    "      lr=0.00005, \n",
    "      epochs=50, \n",
    "      prune_amount=0.1, \n",
    "      prune_steps=3, \n",
    "      out_dir='arxiv_distilbert'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Example usage of Qwen2.5 model fine-tuning using less memory Supervised fine tuning approach of unsloth\n",
    "    model_output_dir = train_llm(model_name=\"unsloth/Qwen2.5-0.5B\", dataset_name=\"mlabonne/FineTome-100k\", max_steps=150, epochs=5, output_dir=\"lora_model\")\n",
    "    \n",
    "    # Example inference usage of fine-tuned model\n",
    "    inference_llm(model_dir=model_output_dir, input_text=\"write a fastapi program with all the routes such as post, get, put, delete make use of in memory data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contextual RAG\n",
    "* Contextual Embeddings and Contextual BM25. This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks.\n",
    "* Prompt Caching Approaches to redude the cost of the API \n",
    "* BM25 refines this by considering document length and applying a saturation function to term frequency, which helps prevent common words from dominating the results.\n",
    "\n",
    "\n",
    "##### Multimodal RAG with vision language models such as ColPali\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_t5():\n",
    "    try:\n",
    "        # !pip install pydantic==1.10.9 nvidia-ml-py3==7.352.0 pytorch-lightning==2.0.1.post0 transformers==4.28.0 torchvision==0.15.1 rouge-score==0.1.2 tensorboardx==2.6 accelerate==0.18.0 deepspeed==0.9.0 peft==0.2.0\n",
    "\n",
    "        # PyTorch autodiff library:\n",
    "        import torch\n",
    "        from torch.utils.data import Dataset\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        # Hugging Face for model, tokenizer, and a training function:\n",
    "        from transformers import T5ForConditionalGeneration\n",
    "        from transformers import AutoTokenizer\n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "        # PyTorch Lightning for easier training:\n",
    "        import pytorch_lightning as pl\n",
    "        from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "        # Magic for viewing TensorBoard in-Notebook:\n",
    "        %load_ext tensorboard\n",
    "\n",
    "        # LoRA for PEFT:\n",
    "        from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "        # Filesystem/data utilities:\n",
    "        import os\n",
    "        import json\n",
    "\n",
    "        ## large portions of this adapted from https://docs.mosaicml.com/projects/streaming/en/stable/examples/synthetic_nlp.html\n",
    "\n",
    "        from typing import Any, Dict, List, Tuple\n",
    "\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        import json, os\n",
    "\n",
    "\n",
    "        ones = (\n",
    "            \"zero one two three four five six seven eight nine ten eleven twelve thirteen fourteen \"\n",
    "            + \"fifteen sixteen seventeen eighteen nineteen\"\n",
    "        ).split()\n",
    "\n",
    "        tens = \"twenty thirty forty fifty sixty seventy eighty ninety\".split()\n",
    "\n",
    "\n",
    "        def say(i: int) -> List[str]:\n",
    "            \"\"\"Get the word form of a number.\n",
    "\n",
    "            Args:\n",
    "                i (int): The number.\n",
    "\n",
    "            Returns:\n",
    "                List[str]: The number in word form.\n",
    "            \"\"\"\n",
    "            if i < 0:\n",
    "                return [\"negative\"] + say(-i)\n",
    "            elif i <= 19:\n",
    "                return [ones[i]]\n",
    "            elif i < 100:\n",
    "                return [tens[i // 10 - 2]] + ([ones[i % 10]] if i % 10 else [])\n",
    "            elif i < 1_000:\n",
    "                return [ones[i // 100], \"hundred\"] + (say(i % 100) if i % 100 else [])\n",
    "            elif i < 1_000_000:\n",
    "                return say(i // 1_000) + [\"thousand\"] + (say(i % 1_000) if i % 1_000 else [])\n",
    "            elif i < 1_000_000_000:\n",
    "                return (\n",
    "                    say(i // 1_000_000)\n",
    "                    + [\"million\"]\n",
    "                    + (say(i % 1_000_000) if i % 1_000_000 else [])\n",
    "                )\n",
    "            else:\n",
    "                assert False\n",
    "\n",
    "\n",
    "        def get_random_number() -> int:\n",
    "            \"\"\"Pick a random number the way humans would.\n",
    "\n",
    "            Picked numbers are positively skewed, exponentially distributed (good for curriculum learning).\n",
    "\n",
    "            Returns:\n",
    "                int: The number.\n",
    "            \"\"\"\n",
    "            sign = (np.random.random() < 0.8) * 2 - 1\n",
    "            mag = 10 ** np.random.uniform(1, 4) - 10\n",
    "            return sign * int(mag**2)\n",
    "\n",
    "\n",
    "        def get_numbers(num_train: int, num_val: int) -> Tuple[List[int], List[int]]:\n",
    "            \"\"\"Get two non-overlapping splits of unique random numbers.\n",
    "\n",
    "            Because the distribution is exponential, we are unlikely to run out of numbers.\n",
    "\n",
    "            Args:\n",
    "                num_train (int): Number of training samples.\n",
    "                num_val (int): Number of validation samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[List[int], List[int]]: The two generated splits.\n",
    "            \"\"\"\n",
    "            total = num_train + num_val\n",
    "            numbers = set()\n",
    "            bar = tqdm(total=total, leave=False)\n",
    "            while len(numbers) < total:\n",
    "                was = len(numbers)\n",
    "                numbers.add(get_random_number())\n",
    "                bar.update(len(numbers) - was)\n",
    "            numbers = list(numbers)\n",
    "            np.random.shuffle(numbers)\n",
    "            return numbers[:num_train], numbers[num_train:]\n",
    "\n",
    "\n",
    "        def generate_samples(numbers: List[int]) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Generate samples from a list of numbers.\n",
    "\n",
    "            Args:\n",
    "                numbers (List[int]): The numbers.\n",
    "\n",
    "            Returns:\n",
    "                List[Dict[str, Any]]: The corresponding samples.\n",
    "            \"\"\"\n",
    "            samples = []\n",
    "            for num in numbers:\n",
    "                words = \" \".join(say(num))\n",
    "                sample = {\"number\": num, \"words\": words}\n",
    "                samples.append(sample)\n",
    "            return samples\n",
    "\n",
    "\n",
    "        def get_dataset(\n",
    "            num_train: int, num_val: int\n",
    "        ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            \"\"\"Generate a number-saying dataset of the given size.\n",
    "\n",
    "            Args:\n",
    "                num_train (int): Number of training samples.\n",
    "                num_val (int): Number of validation samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]: The two generated splits.\n",
    "            \"\"\"\n",
    "            train_nums, val_nums = get_numbers(num_train, num_val)\n",
    "            train_samples = generate_samples(train_nums)\n",
    "            val_samples = generate_samples(val_nums)\n",
    "            return train_samples, val_samples\n",
    "\n",
    "\n",
    "        def create_folder_structure():\n",
    "            if not os.path.isdir(\"./data/\"):\n",
    "                os.mkdir(\"./data/\")\n",
    "            if not os.path.isdir(\"./data/train/\"):\n",
    "                os.mkdir(\"./data/train/\")\n",
    "            if not os.path.isdir(\"./data/val/\"):\n",
    "                os.mkdir(\"./data/val/\")\n",
    "\n",
    "            for f in os.listdir(\"./data/train/\"):\n",
    "                os.remove(os.path.join(\"./data/train\", f))\n",
    "            for f in os.listdir(\"./data/val/\"):\n",
    "                os.remove(os.path.join(\"./data/val\", f))\n",
    "\n",
    "\n",
    "        def getData(num_train: int, num_val: int):\n",
    "            print(f\"Generating synthetic dataset ({num_train} train, {num_val} val)...\")\n",
    "            train_samples, val_samples = get_dataset(num_train, num_val)\n",
    "\n",
    "            create_folder_structure()\n",
    "\n",
    "            for i in range(len(train_samples)):\n",
    "                with open(f\"./data/train/{i}.json\", \"w\") as outfile:\n",
    "                    json.dump(train_samples[i], outfile)\n",
    "\n",
    "            for j in range(len(val_samples)):\n",
    "                with open(f\"./data/val/{j}.json\", \"w\") as outfile:\n",
    "                    json.dump(val_samples[j], outfile)\n",
    "\n",
    "        # if __name__ == \"__main__\":\n",
    "        #     # Number of training and validation samples\n",
    "        #     num_train_samples = 10_000  # 10k samples\n",
    "        #     num_val_samples = 2000  # 2k samples\n",
    "\n",
    "        #     # Create the samples.\n",
    "        #     main(num_train_samples, num_val_samples)\n",
    "        \n",
    "        MODEL_NAME = 't5-small'\n",
    "        getData(num_train=1000, num_val=200)\n",
    "        print('./data/train/0.json')\n",
    "        print('./data/train/10.json')\n",
    "        class StreamingDataset(Dataset):\n",
    "            def __init__(self, path):\n",
    "                self.path = path\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained( # automatically detects T5 and tokenizes for T5\n",
    "                    MODEL_NAME, cache_dir='./cache/', use_fast=True\n",
    "                )\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(os.listdir(self.path))\n",
    "\n",
    "            def encode_text(self, context, text):\n",
    "                ctext = str(context)\n",
    "                ctext = \" \".join(ctext.split())\n",
    "                text = str(text)\n",
    "                text = \" \".join(text.split())\n",
    "                source = self.tokenizer.batch_encode_plus(\n",
    "                    [ctext],\n",
    "                    max_length=16,\n",
    "                    truncation=True,\n",
    "                    # pad_to_max_length=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                target = self.tokenizer.batch_encode_plus(\n",
    "                    [text],\n",
    "                    max_length=16,\n",
    "                    truncation=True,\n",
    "                    # pad_to_max_length=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                y = target[\"input_ids\"]\n",
    "                target_id = y[:, :-1].contiguous()\n",
    "                target_label = y[:, 1:].clone().detach()\n",
    "                target_label[\n",
    "                    y[:, 1:] == self.tokenizer.pad_token_id\n",
    "                ] = -100  # in case the labels are not provided, empty string\n",
    "                return source[\"input_ids\"], source[\"attention_mask\"], target_id, target_label\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                file_path = os.path.join(self.path, str(idx) + \".json\")\n",
    "                with open(file_path, \"r\") as infile:\n",
    "                    data = json.load(infile)\n",
    "                number, words = str(data[\"number\"]), data[\"words\"]\n",
    "                return self.encode_text(number, words)\n",
    "        train_data = StreamingDataset('./data/train/')\n",
    "        val_data = StreamingDataset('./data/val')\n",
    "        # Deconstruct individual-data-point tuples and collates into batch variables:\n",
    "        def collate_fn(batch):\n",
    "            input_ids = torch.stack([torch.flatten(x[0]) for x in batch])\n",
    "            sequence_mask = torch.stack([torch.flatten(x[1]) for x in batch])\n",
    "            target_ids = torch.stack([torch.flatten(x[2]) for x in batch])\n",
    "            target_label = torch.stack([torch.flatten(x[3]) for x in batch])\n",
    "            return input_ids, sequence_mask, target_ids, target_label\n",
    "        class T5Finetuner(pl.LightningModule):\n",
    "            def __init__(self, model, args, train_data, val_data):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "                self.args = args\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    MODEL_NAME, cache_dir='./cache/', use_fast=True\n",
    "                )\n",
    "                self.train_data, self.val_data = train_data, val_data\n",
    "\n",
    "            def forward(self, batch, batch_idx):\n",
    "                source_ids, source_mask, target_ids, target_labels = batch\n",
    "                return self.model(\n",
    "                    input_ids=source_ids,\n",
    "                    attention_mask=source_mask,\n",
    "                    decoder_input_ids=target_ids,\n",
    "                    labels=target_labels,\n",
    "                )\n",
    "\n",
    "            def training_step(self, batch, batch_idx):\n",
    "                loss = self(batch, batch_idx)[0] # self() is forward() in Lightning\n",
    "                self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "                return loss\n",
    "\n",
    "            def validation_step(self, batch, batch_idx): # No backprop during validation\n",
    "                loss = self(batch, batch_idx)[0]\n",
    "                self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "                return loss\n",
    "\n",
    "            # Data loaders provide clever optimizations like pre-fetching next-needed training data point on additional worker\n",
    "            def train_dataloader(self):\n",
    "                return DataLoader(\n",
    "                    self.train_data,\n",
    "                    batch_size=self.args['batch_size'],\n",
    "                    num_workers=os.cpu_count(),\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=collate_fn,\n",
    "                    prefetch_factor=128, # number of samples to prefetch\n",
    "                )\n",
    "\n",
    "            def val_dataloader(self):\n",
    "                return DataLoader(\n",
    "                    self.val_data,\n",
    "                    batch_size=self.args['batch_size'],\n",
    "                    num_workers=os.cpu_count(),\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=collate_fn,\n",
    "                    prefetch_factor=128,\n",
    "                )\n",
    "\n",
    "            def configure_optimizers(self):\n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    self.trainer.model.parameters(), lr=self.args['lr'], weight_decay=0.01\n",
    "                )\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=5, # initial, slow LR steps can \"warm up\" attention\n",
    "                    num_training_steps=self.args['epochs']\n",
    "                    * len(self.train_data)\n",
    "                    / self.args['batch_size'],\n",
    "                )\n",
    "                return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        tensorboard_logger = TensorBoardLogger('tb_logs', name='t5_finetuner')\n",
    "        # Instantiate Hugging Face model:\n",
    "        hg_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir='./cache/')\n",
    "\n",
    "        # Training arguments:\n",
    "        args = {'batch_size': 256, 'epochs': 100, 'lr': 1e-4}\n",
    "\n",
    "        # Train with PyTorch Lightning:\n",
    "        pl_model = T5Finetuner(hg_model, args, train_data, val_data)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=args['epochs'],\n",
    "            log_every_n_steps=4, # this defaults to 50 but we have so few training data\n",
    "            logger=tensorboard_logger,\n",
    "        )\n",
    "        trainer.fit(pl_model)\n",
    "        # Inferencing the model\n",
    "        val_ids, val_mask, _, _ = val_data[0]\n",
    "        generated_ids = pl_model.model.generate(input_ids=val_ids, attention_mask=val_mask, max_new_tokens=16)\n",
    "        prediction =  [\n",
    "              pl_model.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "              for g in generated_ids\n",
    "              ]\n",
    "        print(prediction)\n",
    "\n",
    "        # Instantiate Hugging Face model with PEFT:\n",
    "        hg_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir='./cache/')\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "            inference_mode=False, # training mode\n",
    "            r=8, # rank of LORA parameters; smaller r means fewer parameters\n",
    "            lora_alpha=32, # scales LoRA reparameterization; helps with generalization; value from huggingface.co/blog/peft\n",
    "            lora_dropout=0.1, # dropout probability for LoRA layers; ditto\n",
    "        )\n",
    "        hg_model = get_peft_model(hg_model, peft_config)\n",
    "        hg_model.print_trainable_parameters()\n",
    "        # Training arguments:\n",
    "        args = {'batch_size': 4, 'epochs': 100, 'lr': 1e-4}\n",
    "\n",
    "        # Train with PyTorch Lightning:\n",
    "        pl_model = T5Finetuner(hg_model, args, train_data, val_data)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=args['epochs'],\n",
    "            precision=\"16-mixed\",\n",
    "            accumulate_grad_batches=4,\n",
    "        )\n",
    "        trainer.fit(pl_model)\n",
    "        # Inferencing the PEFT odel\n",
    "        val_ids, val_mask, _, _ = val_data[0]\n",
    "        generated_ids = pl_model.model.generate(input_ids=val_ids, attention_mask=val_mask, max_new_tokens=16)\n",
    "        prediction =  [\n",
    "              pl_model.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "              for g in generated_ids\n",
    "              ]\n",
    "        print(prediction)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
